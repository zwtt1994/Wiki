<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2015-Effective Approaches to Attention-based Neural Machine Translation - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#05-NLP">05-NLP</a>&nbsp;&#187;&nbsp;2015-Effective Approaches to Attention-based Neural Machine Translation
    <span class="updated">Page Updated&nbsp;
      2020-07-25
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2015-Effective Approaches to Attention-based Neural Machine Translation</div>

  <h2 id="_1">总结</h2>
<ul>
<li>对基于attention的seq2seq进行了改进。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>Global attention机制<br />
    <div style="text-align: center"><img src="/wiki/attach/images/AttentionV2-01.png" style="max-width:400px"></div></p>
<ul>
<li>重新定义了encode信息聚合结构的计算方式，新的decode向量由context向量和decode输出的非线性变化得到。<br />
<div style="text-align: center"><img src="/wiki/attach/images/AttentionV2-00.png" style="max-width:200px"></div></li>
<li>尝试了多种attention权值计算方式。<br />
<div style="text-align: center"><img src="/wiki/attach/images/AttentionV2-02.png" style="max-width:200px"></div></li>
</ul>
</li>
<li>
<p>Local attention机制<br />
    <div style="text-align: center"><img src="/wiki/attach/images/AttentionV2-04.png" style="max-width:400px"></div></p>
<ul>
<li>全局计算代价太高，例如翻译全篇文章，所以只选择一部分进行计算。</li>
<li>尝试了两种位置选择方式，一种是直接选择decode相应位置的encode位置为中心位置，另一种是通过参数预测对齐。</li>
</ul>
</li>
<li>
<p>Input-feeding Approach，将t时刻decode的输出+隐层状态一起输入到t+1的decode，之前的模型只使用了decode的输出。<br />
    <div style="text-align: center"><img src="/wiki/attach/images/AttentionV2-05.png" style="max-width:400px"></div></p>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2021 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2021-06-19 15:14:48</p>
      </span>
    </div>

    
    
  </body>
</html>