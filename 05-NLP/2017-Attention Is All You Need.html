<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2017-Attention Is All You Need - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#05-NLP">05-NLP</a>&nbsp;&#187;&nbsp;2017-Attention Is All You Need
    <span class="updated">Page Updated&nbsp;
      2020-07-25，update on 2023-12-24
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2017-Attention Is All You Need</div>

  <h2 id="_1">总结</h2>
<ul>
<li>rnn的方法存在长度处理限制，时间顺序规则导致计算效率较低，同时注意力机制也被引入到各模型中来提升模型表达能力。</li>
<li>一些工作利用cnn来并行计算不同位置间的相关信息，但往往计算复杂度和输入长度呈线性或者指数增长关系。</li>
<li>本文提出transformer结构，利用self-attention来解决并行计算和长度限制的问题。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>模型结构<br />
<div style="text-align: center"><img src="/wiki/attach/images/Transformer-update-01.png" style="max-width:400px"></div></p>
<ul>
<li>编码器由多个子层堆叠而成，每个子层包含两部分，一部分是multi-head attention，一部分是两层全链接FFN，两个部分都加入了残差链接和layer normalization，因此所有子层输出维度一致。</li>
<li>为什么要用残差网络：网络很难学习到恒等映射，而容易学习残差，解决梯度消失问题</li>
<li>Layer normalization：层神经元维度的归一化；Batch normalization：样本维度的归一化</li>
<li>解码器：Masked multi-head attention + multi-head attention +FFN，mask的目的是并行计算时不让时序前面的建模看到后面的信息</li>
</ul>
</li>
<li>
<p>multi-head attention<br />
<div style="text-align: center"><img src="/wiki/attach/images/Transformer-update-02.png" style="max-width:600px"></div><br />
<div style="text-align: center"><img src="/wiki/attach/images/Transformer-update-03.png" style="max-width:500px"></div><br />
<div style="text-align: center"><img src="/wiki/attach/images/Transformer-update-04.png" style="max-width:800px"></div></p>
</li>
<li>
<p>FFN：两层全链接，relu+无激活函数</p>
</li>
<li>
<p>输入和输出embedding和softmax用同一套，在embedding层对向量乘以维度的开方</p>
</li>
<li>
<p>Positional Encoding，位置编码：由于模型不含递归和卷积，需要加入额外标记让模型学习到位置信息；论文尝试了可学习的和三角函数，发现两者效果类似，因此选择了更鲁棒的三角函数</p>
</li>
<li>
<p>Transformer的优点：self-attention和rnn/cnn的对比，计算量更小，并行度更高</p>
</li>
<li>
<p>正则化：dropout（子层和embedding），label smothing</p>
</li>
<li>
<p>Transformer的缺点：有些rnn轻易可以解决的问题（如序列复制），transformer很难实现；Positional Encoding的存在会导致在处理训练中没碰到过的句子长度（特别长）时表现很差。</p>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2024 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2024-02-22 17:26:18</p>
      </span>
    </div>

    
    
  </body>
</html>