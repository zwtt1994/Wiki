<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2020-深度推荐系统 - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#02-Recommend System">02-Recommend System</a>&nbsp;&#187;&nbsp;2020-深度推荐系统
    <span class="updated">Page Updated&nbsp;
      2020-06-14
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2020-深度推荐系统</div>

  <h2 id="_1">第一章</h2>
<ul>
<li>推荐系统的意义：帮助用户在"信息过载"的情况下高效地过去兴趣信息；帮助公司做好用户增长，提高用户LTV，从而提升整体价值。</li>
<li>推荐系统的架构主要分为数据架构和模型架构。<ul>
<li>数据架构部分负责信息的收集与处理，包括离线和实时框架，该部分主要负责样本、特征和数据监控等。</li>
<li>模型架构部分是推荐系统的主体，负责推荐物料的召回与排序，并提供离线评估与在线AB测试等评估模块。</li>
</ul>
</li>
</ul>
<h2 id="-">第二章 - 深度学习之前的推荐系统</h2>
<ul>
<li>协同过滤（Collaborative Filtering）<ul>
<li>将"共现矩阵中的向量"通过"相似度计算方法"计算得到相似度，并利用相似度加权将评分求和得到目标值。</li>
<li>基于用户的cf/基于item的cf，分别对应基于用户相似度/基于物品的相似度进行推荐。</li>
</ul>
</li>
<li>矩阵分解（Matrix Factorization）<ul>
<li>定义用户向量矩阵与item向量矩阵，两者矩阵乘积为共现矩阵。</li>
<li>利用交替最小二乘（ALS）方法求解，并可以设置正则项。</li>
</ul>
</li>
<li>逻辑回归（Logistic Regression)<ul>
<li>本质上是利用线性回归学习几率。</li>
<li>优点是结构简单，可解释性强；缺点是表达能力不强。</li>
</ul>
</li>
<li>FM与FFM<ul>
<li>FM能够学习特征的交叉信息，详见FM论文笔记。</li>
<li>FFM引入了Filed的概念，特征被划分为多个Filed，每个特征对与不同的Field设置不同的隐向量。</li>
</ul>
</li>
<li>GBDT+LR<ul>
<li>本质上是利用GBDT来做特征工程，再用LR模型做训练。</li>
<li>GBDT输出的是样本落入各决策树叶子节点的编号向量。</li>
</ul>
</li>
<li>混合逻辑回滚（MLR）<ul>
<li>本质上对于不同的数据利用不同LR函数来拟合。</li>
<li>例如利用softmax函数来对所有的LR函数输出做加权求和，有点像attention机制。</li>
</ul>
</li>
</ul>
<h2 id="-_1">第三章 - 深度学习在推荐系统中的应用</h2>
<ul>
<li>自编码器（AutoRec）<ul>
<li>三层神经网络，输入层是评分向量，训练使得输出层和输入层的误差最小（只考虑输入层有值的误差），输出层即是预测的评分向量。</li>
</ul>
</li>
<li>Deep Cross模型<ul>
<li>特征embedding后拼接并进入mlp，相比之前的突破点是特征embedding。</li>
</ul>
</li>
<li>NeuralCF<ul>
<li>矩阵分解其实就是使得user向量和item向量内积为评分值，NeuralCF将内积操作改成mlp。</li>
<li>这里能联想到DSSM的结构，矩阵分解是直接内积，DSSM是先将两个向量经过mlp再内积。</li>
<li>NeuralCF和矩阵分解是直接对id进行embedding，而DSSM考虑了其他的特征。</li>
</ul>
</li>
<li>PNN（product）<ul>
<li>特征embedding后进行交叉内积并进入mlp，相当于是在FM上加mlp。</li>
<li>乘积层包括内积和外积操作，为了减小模型负担，该模型提出将所有特征的外积矩阵求和，这个操作会模糊掉特征中的信息，一般不建议使用。</li>
</ul>
</li>
<li>Wide&amp;deep<ul>
<li>主要目的是同时利用先验信息和潜在信息，详见Wide&amp;deep论文笔记。</li>
<li>之后的很多论文都是在此基础上改进wide/deep部分的结构。</li>
</ul>
</li>
<li>DCN（deep&amp;cross)<ul>
<li>利用cross结构代替wide部分，cross结构利用向量外积将特征充分交叉，并在每一层都保留了输入向量，模型的非线性学习能力更强。</li>
<li>我理解这边可以将特征外积与输入向量分开，这样可以更针对性的学习特征交叉信息。</li>
</ul>
</li>
<li>FM和深度学习的结合<ul>
<li>FNN，为了加速embedding部分的收敛速度，利用FM做预训练，将FM各特征的隐向量来初始化dnn的特征embedding参数。</li>
<li>DeepFM，将FM结构代替wide部分，同时具备了特征低阶交叉和高阶交叉，并利用embedding解决了稀疏特征的参数规模较大问题。</li>
<li>NFM，FM二阶交叉部分用的是内积，在NFM中用了向量元素乘法保留了整个向量，并将所有交叉向量求和输出到mlp中。</li>
</ul>
</li>
<li>attention在推荐模型中的应用<ul>
<li>AFM，在特征交叉层之后加入attention结构，对下一层的输出进行加权求和，是模型结构上的一次尝试。</li>
<li>DIN，针对业务特点在用户历史行为中加入attention结构，详见DIN笔记。</li>
<li>DIEN，引入用户历史行为的"序列信息"，详见DIEN笔记。</li>
</ul>
</li>
<li>强化学习和推荐系统的结合<ul>
<li>介绍的比较简略。</li>
</ul>
</li>
</ul>
<h2 id="-embedding">第四章 - embedding在推荐系统中的应用</h2>
<ul>
<li>Word2vec<ul>
<li>经典的embedding技术，详见word2vec笔记。</li>
</ul>
</li>
<li>Item2vec - word2vec在推荐领域的推广<ul>
<li>word中序列信息在item中可以理解为用户的item行为序列，与word不同的点是，item在embedding的时候可以不考虑时序信息。</li>
<li>广义的item2vec，类似与DSSM的双塔模型。</li>
<li>item2vec的缺点是只能利用序列型的数据来建立item间的相关性。</li>
</ul>
</li>
<li>Graph embedding - 引入更多的信息<ul>
<li>DeepWalk：在物品组成的图上随机游走产生item序列，利用word2vec方法训练。</li>
<li>Node2vec：与DeepWalk相比优化来游走方法，通过限制转移概率来权衡BFS（结构性相似）和DFS（节点内容相似）。</li>
<li>GES：word2vec模型输入除了itemId还增加了brandId，catId等embedding，可以训练得到多个维度的embedding，最后求平均得到最后的向量，能够一定程度上解决冷启动问题。</li>
<li>EGES：考虑到不同的类别embedding对结果贡献不同，将上述求平均替换为attention的结构。</li>
</ul>
</li>
<li>embedding与推荐系统的结合<ul>
<li>利用embedding预训练，作为特征的embedding层。</li>
<li>利用embedding向量的相似度做召回。</li>
</ul>
</li>
<li>局部敏感hash<ul>
<li>基本思想：在高维数据空间中的两个相邻的数据被映射到低维数据空间中后，将会有很大的概率任然相邻。通过这样一映射，我们可以在低维数据空间来寻找相邻的数据点，减少搜索复杂度，有这样性质的哈希映射称为是局部敏感的。</li>
<li>数据描述：若d(x1,x2)<r1，那么P[h(x1)=h(x2)]≥p1；若d(x1,x2)>r2，那么P[h(x1)=h(x2)]≤p2。即当x1和x2的相似度与hash之后在同一个桶里的概率成正相关。</li>
<li>建立索引：将数据通过一组LSH函数进行降维得到"低维矩阵"，将低维矩阵的特征维度切分为若干组，每一组通过一个常规的哈希函数进行映射（相似度判断），此时如果映射的是同一个桶说明样本在这组特征上是相似的。</li>
<li>在线检索：将数据进过LSH方法得到桶号，低维矩阵特征维度被分为若干组，只要这些组中有n个以上是在同一个桶即认为样本在同一个桶，此时取出同一桶内的数据做判断即可（为了进一步减少计算量可以随机取n个）。</li>
</ul>
</li>
</ul>
<h2 id="-_2">第五章 - 多角度审视推荐系统</h2>
<ul>
<li>特征工程<ul>
<li>详见特征工程笔记。</li>
</ul>
</li>
<li>召回层的主要策略<ul>
<li>热门、历史行为、lbs和协同过滤等，可以理解为和用户相关的一阶（历史行为）、二阶（关注人的行为）或多阶的召回策略。</li>
<li>基于embedding召回，将用户/item向量化，通过计算向量相似度来召回。</li>
</ul>
</li>
<li>推荐系统的实时性<ul>
<li>特征和模型都需要具备实时性。</li>
</ul>
</li>
<li>合理设置推荐系统的优化目标<ul>
<li>模型优化目标应与应用场景统一，如Youtube以用户观看时长为优化目标。</li>
</ul>
</li>
<li>推荐系统中比模型结构更重要的东西<ul>
<li>推荐系统的模型结构不是通用的，需要根据场景的先验知识来设计模型结构，如观察用户行为并在模型中加入有价值的用户信息。</li>
<li>算法工程师需要对用户的角度思考问题，而不是只做调参侠。</li>
</ul>
</li>
<li>冷启动的解决方法<ul>
<li>基于规则的冷启动。</li>
<li>补充冷启动中的用户和物品特征。</li>
<li>利用迁移学习、主动学习与EE。</li>
</ul>
</li>
<li>Exploit &amp; Explore<ul>
<li>e-greedy，以某个概率选择目前最优，反之随机选择。</li>
<li>汤姆森采样，利用beta分布来采样，将不确定性与累积样本的数量结合。</li>
<li>UCB，同样是考虑了累积样本数量，但利用了霍夫丁不等式，所以不需要做采样。</li>
<li>LinUCB，通过线性预测来估计得分。</li>
</ul>
</li>
</ul>
<h2 id="-_3">第六章 - 深度学习推荐系统的工程实现</h2>
<ul>
<li>推荐系统的数据流<ul>
<li>批处理与流计算两种大数据架构，分别对应离线与实时两种数据处理方式。</li>
</ul>
</li>
<li>分布式离线训练架构<ul>
<li>Spark MLlib、PS、TF，待补充笔记</li>
</ul>
</li>
<li>深度学习模型的上线部署<ul>
<li>预存embedding结果+轻量级线上模型。</li>
<li>PMML转化并部署模型。</li>
<li>TF serving。</li>
</ul>
</li>
</ul>
<h2 id="-_4">第七章 - 推荐系统的评估</h2>
<ul>
<li>离线评价指标</li>
<li>AB测试与线上评估指标</li>
</ul>
<h2 id="-_5">第八章 - 深度学习的前沿实践</h2>
<ul>
<li>介绍了Facebook DLRM，Airbnb的embedding，Youtube的视频推荐系统和阿里巴巴的推荐系统，详见笔记。</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2024 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2024-03-25 11:20:23</p>
      </span>
    </div>

    
    
  </body>
</html>