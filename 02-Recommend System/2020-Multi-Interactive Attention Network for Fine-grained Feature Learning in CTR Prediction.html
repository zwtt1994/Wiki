<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2020-Multi-Interactive Attention Network for Fine-grained Feature Learning in CTR Prediction - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#02-Recommend System">02-Recommend System</a>&nbsp;&#187;&nbsp;2020-Multi-Interactive Attention Network for Fine-grained Feature Learning in CTR Prediction
    <span class="updated">Page Updated&nbsp;
      2021-01-16
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2020-Multi-Interactive Attention Network for Fine-grained Feature Learning in CTR Prediction</div>

  <h2 id="_1">总结</h2>
<ul>
<li>推荐系统中为了更好建模用户偏好，可以添加目标item与其他各类特征的交叉信息，本文用的是transformer与self-attention，如果考虑性能问题的话哈达玛积的性价比较高。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>推荐系统常用用户历史行为序列来挖掘用户兴趣，建模的方式一般是引入历史item与目标item的相关性。</p>
</li>
<li>
<p>但目标item与用户与环境的关系也十分重要，通过多塔建模并且添加目标item和用户特征以及环境特征的交叉信息是十分有效的。</p>
</li>
<li>
<p>网络结构如下，在网络交叉部分，历史item与目标item利用了Pre—LN Transformer，其余两个则利用了self-attention。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Multi-Interactive-01.png" style="max-width:700px"></div></p>
</li>
<li>
<p>实验证明了网络的有效性，结果如下；同时做了线上abtest实验，对ctr带来了0.41%的增益。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Multi-Interactive-02.png" style="max-width:700px"></div></p>
</li>
<li>
<p>验证了Pre-LN Transformer的有效性；同时选取了14个case可视化了global attention层分布，验证了新增交叉部分的有效性。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Multi-Interactive-02.png" style="max-width:500px"></div></p>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2023 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2023-12-15 17:06:14</p>
      </span>
    </div>

    
    
  </body>
</html>