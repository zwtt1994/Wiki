<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2020-Controlling Fairness and Bias in Dynamic Learning-to-Rank - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#02-Recommend System">02-Recommend System</a>&nbsp;&#187;&nbsp;2020-Controlling Fairness and Bias in Dynamic Learning-to-Rank
    <span class="updated">Page Updated&nbsp;
      2021-02-16
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2020-Controlling Fairness and Bias in Dynamic Learning-to-Rank</div>

  <h2 id="_1">总结</h2>
<ul>
<li>本文提出了在动态排序中，常规的推荐系统存在"用户偏好描述不准确"和"曝光不公平"的情况，导致排序结果存在问题或者不准确，所以排序算法需要具备"无偏性"和"公平性"。</li>
<li>在公平性中，本文定义了物料"单位偏好下的期望曝光（或者其他描述量）"来进行建模；在无偏性中，本文对公平性建模中涉及到的变量进行了无偏估计推导。</li>
<li>利用公平性的建模结果，本文提出了公平性动态控制的方法，并进行了实验验证。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>问题定义</p>
<ul>
<li>推荐系统中的排序模型是基于曝光样本来学习的，其物料分布与真实物料分布不同，会造成偏差效应。</li>
<li>所以我们需要排序算法具备无偏性（用户偏好定义的准确性）和公平性（对曝光进行合理的分配）。</li>
<li>文中针对的是动态排序：将用户的反馈信息实时地作用到计算排序得分上。</li>
</ul>
</li>
<li>
<p>符号说明</p>
<ul>
<li>用户信息:x</li>
<li>时间因子:t</li>
<li>待排序物料:d</li>
<li>物料是否曝光:e</li>
<li>物料曝光概率:p</li>
<li>用户与物料相关性:r</li>
<li>用户对物料偏好:R</li>
<li>排序规则:π</li>
<li>排序得分:σ</li>
<li>用户正反馈(用户偏好):c</li>
</ul>
</li>
<li>
<p>动态排序</p>
<ul>
<li>用户偏好c的描述如下，用户偏好定义为曝光物料与用户的相关性。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-01.png" style="max-width:300px"></div></li>
<li>由于在排序的时候无法直接获取物料是否曝光，所以可以利用排序得分来估计曝光概率。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-02.png" style="max-width:150px"></div></li>
<li>排序逻辑如下，即利用利用用户特征x计算用户与文档d的相似度，并进行排序；对于动态排序，相似度R与用户反馈c随时间相关即可。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-03.png" style="max-width:250px"></div></li>
</ul>
</li>
<li>
<p>公平性</p>
<ul>
<li>公平性是指：如何公平合理地对物料进行曝光；本文对"判断是否曝光"与"衡量用户对物料的偏好"进行了建模。</li>
<li>定义曝光概率为排序得分、用户信息和用户与物料相关性的边缘分布；<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-04.png" style="max-width:250px"></div></li>
<li>对同一类型的物料进行聚合，得到某类物料的平均曝光度量；<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-05.png" style="max-width:250px"></div></li>
<li>同时定义了用户对整类物料的偏好度量；<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-06.png" style="max-width:250px"></div></li>
<li>由上述两式可以定义出公平性的度量，即某一类物料单位偏好下的期望曝光；此外可以定义类似的公平性度量，如单位偏好下的期望点击。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-07.png" style="max-width:450px"></div></li>
</ul>
</li>
<li>
<p>无偏性</p>
<ul>
<li>无偏性是指用户偏好定义的准确性，文中共涉及到三个变量：位置偏差p，用户对物料的平均相关性R(d|x)，物料全局的期望相关性R(d)；</li>
<li>作者认为位置偏差p的预估不是动态排序中存在的问题，最简单的做法是利用排序得分σ来确定，也可以加入其他更多的特征。</li>
<li>对R(d|x)的预估类似于我们平常预估的ctr，但由于用户反馈对用户物料相关性的估计不是无偏的，所以需要进行调整，本质上是利用曝光概率p和用户反馈c来无偏估计相关度r。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-08.png" style="max-width:550px"></div><br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-09.png" style="max-width:350px"></div></li>
<li>所以可以通过下面这个损失函数来无偏估计用户和物料的相关度，实现无偏排序。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-10.png" style="max-width:400px"></div></li>
<li>物料全局相关性的无偏估计如下，即用户反馈c/曝光概率p，其证明也是直觉的。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-11.png" style="max-width:250px"></div></li>
</ul>
</li>
<li>
<p>公平性动态控制</p>
<ul>
<li>上文已经提到不公平性的平均度量为两个类目之间的不公平度，全局不公平度则是所有类目见不公平度的均值。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-12.png" style="max-width:300px"></div></li>
<li>为了降低上述不公平度，本文提出计算如下误差，并利用误差对排序得分进行调整。同时，根据线上新的实验结果重新计算上述不公平度对误差进行更新，直到收敛为止。<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-13.png" style="max-width:400px"></div><br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-14.png" style="max-width:300px"></div></li>
</ul>
</li>
<li>
<p>实验验证</p>
<ul>
<li>Can FairCo reduce unfairness while maintaining good ranking quality?<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-15.png" style="max-width:400px"></div></li>
<li>Do the unbiased estimates converge to the true relevances?<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-16.png" style="max-width:400px"></div></li>
<li>Does FairCo overcome the rich-get-richer dynamic? <br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-17.png" style="max-width:400px"></div></li>
<li>How effective is the FairCo compared to a more expensive Linear-Programming Baseline?<br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-18.png" style="max-width:400px"></div></li>
<li>Is FairCo effective for different group sizes? <br />
<div style="text-align: center"><img src="/wiki/attach/images/fair-19.png" style="max-width:400px"></div></li>
<li>Is FairCo effective for different user distributions?<br />
 <div style="text-align: center"><img src="/wiki/attach/images/fair-20.png" style="max-width:400px"></div></li>
<li>Does personalization via unbiased objective improve NDCG?<br />
 <div style="text-align: center"><img src="/wiki/attach/images/fair-21.png" style="max-width:400px"></div></li>
<li>Can FairCo reduce unfairness?<br />
 <div style="text-align: center"><img src="/wiki/attach/images/fair-22.png" style="max-width:400px"></div></li>
<li>How different are exposure and impact fairness? <br />
 <div style="text-align: center"><img src="/wiki/attach/images/fair-23.png" style="max-width:400px"></div></li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2023 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2023-12-25 12:56:41</p>
      </span>
    </div>

    
    
  </body>
</html>