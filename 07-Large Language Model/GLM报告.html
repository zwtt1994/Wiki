<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>GLM报告 - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#07-Large Language Model">07-Large Language Model</a>&nbsp;&#187;&nbsp;GLM报告
    <span class="updated">Page Updated&nbsp;
      2024-01-14
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">GLM报告</div>

  <h2 id="_1">主要内容</h2>
<ul>
<li>
<p>模型架构 </p>
<ul>
<li>训练范式：单向、双向（词、句子、文档mask、混合去噪器UL2）</li>
<li>layerNorm：post-LN、pre-LN、sandwich-LN、deepNet </li>
<li>position embbeding：三角、可学习、相对位置编码(ALiBi、RoPE)</li>
</ul>
</li>
<li>
<p>模型训练 </p>
<ul>
<li>成本高：GPT3成本1200万美元，GPU </li>
<li>内存占用：激活函数占大部分内存（在forward中保存，用于反向传播），其次是optimizer【todo】和model</li>
<li>混合精度：梯度更新【todo】时用更高精度 </li>
<li>激活函数重演，时间换空间，前向计算时不保留激活函数结果，反向传播时重新计算，可以选择性使用 </li>
<li>数据并行：Param Server（负载均衡问题），All-Reduce分布式随机梯度下降【todo】，ZeRO（拆分optimizer states，gradients，model weight） </li>
<li>模型并行：模型放不下一张显卡，张量并行（分模块拆卡），流水线并行（分阶段拆卡）（GPipe，1F1B）【todo】 </li>
<li>175B参数需要2.8TB显存【todo】，单卡40GB；张量并行+流水线并行+ZeRO-3 </li>
<li>训练稳定性：<ul>
<li>权衡利弊（高精度低效、低精度高效）；OPT-175B训练崩溃时反复调整学习率，跳过数据</li>
<li>BLOOM 176B，emb norm和BF16；</li>
<li>GLM-130B，softmax in 32 避免上下溢出，利用DeepNorm，调小emb层梯度 </li>
</ul>
</li>
</ul>
</li>
<li>
<p>模型量化</p>
<ul>
<li>模型量化是指将神经网络的浮点算法转换为定点</li>
<li>模型规模超过10B时候，activation中会出现outlier异常值（似乎是softmax机制的关键）</li>
<li>GLM-130B，GeGLU激活函数使得关键特性失效，30%左右的维度会出现outlier； Vector-wise对称PTQ量化，INT8几乎不损失，INT4极小损失但显存下降75%</li>
</ul>
</li>
<li>
<p>GLM-130B到chatGLM </p>
<ul>
<li>prompt提示：模型对其很敏感，chat类模型主要为了降低prompt难度，基座模型很难直观理解输入要干什么 </li>
<li>RLHF【todo】：互联网预料质量并非符合人类偏好，通过人类反馈将模型分布调整到人类偏好的高质量数据分布上；Learning to summarize from human feedback(2020)；不仅仅是生成，还有评估和推理能力。 </li>
<li>chatGLM：对话能力，【todo下载下来看看】</li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2024 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2024-10-08 09:30:04</p>
      </span>
    </div>

    
    
  </body>
</html>