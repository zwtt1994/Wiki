<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>LLaMA-META系列 - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#07-Large Language Model">07-Large Language Model</a>&nbsp;&#187;&nbsp;LLaMA-META系列
    <span class="updated">Page Updated&nbsp;
      2024-09-28
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">LLaMA-META系列</div>

  <h2 id="_1">主要内容</h2>
<ul>
<li>LLaMA1(2022)，基础模型<ul>
<li>摘要：用公开数据集就可以训练出优异的基础语言模型，特别的是LLaMA-13B与GPT3-175B表现相当</li>
<li>参数范围：7B-65B（B=10的9次方），serving时小模型速度更快，LLaMA可在单GPU上运行</li>
<li>训练数据：1.4T个tokens（T=10的12次方），使用了CommonCrawl（67%）、C4（15%）、Wikipedia（4.5%)、Books(4.5%)、ArXiv(2.5%)等，Wiki和Books质量高会训练2次<ul>
<li>数据处理：行/书级别去重、ngram语言模型过滤低质内容</li>
</ul>
</li>
<li>网络架构：基于Transformer做了一些改进，上下文长度2k<ul>
<li>pre-normalization：收敛更快但上限不如post-normalization，详见论文笔记Layer Normalization</li>
<li>SwiGLU激活函数：GLU和Swish的结合，既能够具备gate的选择性，又使得全域都具备梯度，SwiGLU(x)=x1 * sigmoid(x1) * sigmoid(x2)</li>
<li>RoPE：旋转位置编码，在attention中加入位置参数，详见论文比例Position Embedding</li>
<li>超参数：估算为12 * Tranformer层数 * 隐藏层维度的平方，attention矩阵的维度=隐藏层维度/多头数<ul>
<li>6.7B的模型参数配置为：隐藏层维度4096，multi-attention多头数32，Transformer层数32，学习率0.0003，batch size=4M</li>
</ul>
</li>
</ul>
</li>
<li>Optimizer：AdamW优化器，并叠加10%的余弦式衰减，AdamW相比Adam解决的就是将衰减直接作用到最后的参数更新，而不参与动量的更新<ul>
<li>使用0.1的权重衰减和1.0的梯度裁剪</li>
</ul>
</li>
<li>优化训练速度：<ul>
<li>利用causal multi-head attention的时序特性，减少对mask部分的计算和存储</li>
<li>利用checkpoint存储了计算成本较高的结果，用空间换时间</li>
<li>利用model and sequence parallelism进一步提升上述优化效率</li>
<li>overlap 激活计算和 GPU 之间的网络通信，提升时分复用的效率</li>
<li>65B模型训练速度为380 tokens/second/GPU，因此在2048个A100/80GB/GPU上，训练一次1.4T数据需要21天</li>
</ul>
</li>
<li>模型效果，测试了zero-shot和few-shot（1-64个示例）两种任务，共计20个基准测试<ul>
<li>尝试推理、闭卷问答、阅读理解、数学推理、代码生成、大规模多任务语言理解（MMLU）</li>
<li>在训练过程中模型性能的变化，大部分都是随着训练数据的增加而逐步提升</li>
</ul>
</li>
</ul>
</li>
<li>LLaMA2(2023)，chat版本<ul>
<li>摘要：在预训练大语言模型的基础上，针对对话场景进行了微调优化，可作为闭源模型代替方案</li>
<li>背景：没有能与ChatGPT匹敌的开源大模型，开源LLaMA2/LLaMA2-Chat填补空白</li>
<li>训练数据：2T个tokens（T=10的12次方），组合了公开可用的数据源，去掉了个人信息，对事实类数据源进行上采样</li>
<li>参数范围：LLaMA2和LLaMA2-Chat都是7B-70B（B=10的9次方），上下文长度4k</li>
<li>模型架构：与LLaMA基本类似，改进部分为上下文长度提升至4k，并利用了GQA<ul>
<li>QGA（grouped-query attention）：通过将attention矩阵分组来降低计算复杂度，同时保持注意力机制的表达能力</li>
<li>超参数：AdamW（β1 = 0.9，β2 = 0.95，eps = 10-5），余弦学习率10%衰减，使用0.1的权重衰减和1.0的梯度裁剪。</li>
<li>分词器（Tokenizer）：字节对编码（bytepair encoding），vocabulary size为32k tokens。</li>
</ul>
</li>
<li>训练速度：<ul>
<li>RSC集群（200Gbps InfiniBand + 400W GPU），生产集群（200Gbps RoCE + 350W GPU；RoCE成本更低）</li>
<li>RoCE + 350W GPU 的集群，经过优化的代码能达到 IB + 400W GPU 集群性能的 90%</li>
<li>A100-80GB（400W/350W TDP）机器，总共耗费了 3.3M GPU-hour</li>
</ul>
</li>
<li>预训练模型效果：LLaMA2 70B 在 MMLU 和 GSM8K 上与 GPT-3.5（OpenAI，2023）接近，与 PaLM（540B）相当，与 GPT-4/PaLM-2-L 存在较大差距</li>
<li>训练框架：<ul>
<li>使用公开数据预训练LLaMA2</li>
<li>对LLaMA2进行监督微调（SFT），得到初版LLaMA2-Chat</li>
<li>真人对LLaMA2-Chat的回答进行标注，得到有用性和安全性两个奖励模型</li>
<li>通过RLHF/rejection sampling/PPO，对LLaMA2-Chat进行进一步训练<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-01.png" style="max-width:800px"></div></li>
</ul>
</li>
<li>微调（Fine-tuning），包括指令微调（instruction tuning）和 RLHF，需要大量的计算和标注资源。<ul>
<li>监督式微调（SFT）：只需少量（几万个）高质量 SFT 标注数据就能显著提升结果质量，因此质量相比数量更关键。</li>
<li>微调细节：初始学习率0.00002，权重衰减0.1，batch size=64，上下文长度为4096 tokens，</li>
<li>微调样本：每个样本由一个提示（prompt）和一个回答（answer）组成，为了确保模型序列长度正确填充，将prompt和answer用特殊的token连起来</li>
<li>微调训练：使用自回归目标，并 zero-out the loss on tokens from the user prompt， 因此只在 answer token 上进行反向传播</li>
</ul>
</li>
<li>基于人类反馈的强化学习（RLHF），应用在微调模型之上， 使模型行为与人类偏好和指令进一步对齐。<ul>
<li>人类偏好数据收集：两两对比做程度判断，明显更好/更好/略微好/几乎无差别/不确定，结果会从不同阶段的模型得到确保多样性</li>
<li>安全性数据收集：选中的回答是安全的，另一个回答不安全；两个回答都是安全的；两个回答都不安全</li>
<li>交替更新：LLaMA2-Chat利用标注数据改进的同时，奖励模型需要随着同步更新，否则准确性会迅速下降</li>
<li>奖励模型：<ul>
<li>模型样本：模型的response和prompt（包括前几轮上下文）作为输入，输出标量分数（有效性和安全性分别训练两个模型，因为会冲突）</li>
<li>模型初始化：用LLaMA2-Chat checkpoint初始化，模型架构和超参数一致</li>
<li>训练目标：binary ranking loss，利用了对比学习的思路。<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-02.png" style="max-width:300px"></div></li>
<li>训练细节：数据训练一个epoch，重复训练存在过拟合的情况，学习率70B模型5<em>10-6/其他1</em>10-5，学习率余弦衰减至10%，warm-up 3%数据（size=5），batch-size=512</li>
</ul>
</li>
<li>Iterative Fine-Tuning微调<ul>
<li>真人反馈数据和奖励模型交替更新，因此奖励模型版本会随时间更新</li>
<li>RLHF最关键的两个技术是PPO和Rejection Sampling fine-tuning</li>
<li>在RLHF（V4）之前仅使用拒绝采样微调，之后按顺序结合两者，在再次采样之前，基于最佳采样结果应用PPO进行微调</li>
</ul>
</li>
<li>拒绝采样Rejection Sampling<ul>
<li>仅对的70B LLaMA2-Chat模型上执行拒绝采样，所有较小的模型都在从大模型中拒绝采样的数据上进行微调</li>
<li>每个迭代周期，对于所有prompt从最新的模型中采样k个答案，并用最佳奖励模型评分选择最佳答案</li>
<li>早期只用上一版模型生成答案，但通过迭代发现利用历史所有版本的模型生成答案效果更佳，直观来看不同版本模型可能会随着样本产生能力倾斜</li>
<li>训练流程：对于每一个prompt从模型中采样K个输出，并根据奖励模型选择最佳候选样本，然后使用这些选择的样本进行SFT更新</li>
</ul>
</li>
<li>PPO（Proximal Policy Optimization ）<ul>
<li>详见强化学习笔记</li>
<li>更新流程：基于拒绝采样得到的最佳结果，和价值函数的预估得分，更新模型参数，价值模型预估得分越高，说明模型更应该输出当前结果</li>
<li>训练配置： <ul>
<li>AdamW 优化器，其中 β1 = 0.9、β2 = 0.95、eps = 10−5，0.1 的权重衰减、1.0 的梯度裁剪和 10−6 的恒定学习率</li>
<li>batch size=512、0.2的PPO裁剪阈值、64的小batch size，并且每个小batch size采取一个梯度步骤</li>
<li>对于 7B 和 13B 模型 β = 0.01（KL 惩罚），对于 34B 和 70B 模型 β = 0.005</li>
<li>每个模型200 到 400 次迭代训练，并使用对保留提示的评估来提前停止。70B 模型上的每次 PPO 迭代平均需要 ≈ 330 秒。为了使用大批量快速训练，使用了FSDP。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>多轮一致性的系统消息<ul>
<li>定义系统消息，例如翻译、扮演某人或者赋予爱好，并将其合并到所有用户指令中，例如原始【你今天怎么样】，合成后【扮演拿破仑：你今天怎么样】</li>
<li>为了避免系统消息在每轮对话中重复出现导致的不匹配，我们可以在第一轮之后删除系统消息，并将前几轮的所有标记（包括助手消息）的损失设置为0</li>
</ul>
</li>
<li>发现和认知<ul>
<li>从SFT到RLHF<ul>
<li>论文表示从怀疑到真香，RLHF在效果、成本和时间效率上远超预期</li>
<li>每个人的标注风格也存在显著差异，模型性能受限于最熟练的标注员的能力，RLHF成功的关键是它在标注过程中促进了人和LLM之间的协同，</li>
<li>随着模型的不断迭代，奖励分布逐渐朝着高分漂移<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-03.png" style="max-width:800px"></div></li>
<li>更直观点说，SFT需要给出最佳答案，但RLHF只需要判断哪个答案更好，显然后者更加容易，因此论文认为RLHF是LLM在某些任务中超越人类的关键。</li>
</ul>
</li>
<li>上下文温度重缩放（In-Context Temperature Rescaling），RLHF 学会了根据 prompt 类型适应温度<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-04.png" style="max-width:600px"></div><br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-05.png" style="max-width:600px"></div></li>
<li>时间感知能力（Temporal Perception）<ul>
<li>收集了与特定日期相关的1,000个SFT示例，关键信息是（提问时的日期、事件日期），对时间的概念内化程度超出预期</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>LLaMA3(2024)，多模态<ul>
<li>摘要：能力与GPT-4相当，图像/视频/语音等多模态。</li>
<li>参数范围：8B-405B（B=10的9次方），上下文长度128K，最精确的模型使用了3.8*10e25浮点数进行与预训练，比Llama2的最大版本多近50倍。</li>
<li>训练数据：更谨慎的预训练数据处理和策划流程，以及为后训练数据开发更严格的质量保证和过滤方法，约15T多语言tokens的预训练，Llama2是1.8T。</li>
<li>复杂性：Transformer + 监督微调（SFT） + 拒绝采样（RS）和直接偏好优化（DPO）<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-06.png" style="max-width:800px"></div></li>
<li>预训练<ul>
<li>数据清洗：PII和安全过滤、文本提取和清洗、去重、启发式过滤（去除异常数据）、基于模型的质量过滤（质量分、是否会被维基百科引用等）、代码和推理数据、多语言数据</li>
<li>数据混合：确定预训练数据混合中不同数据源的比例至关重要，知识分类（存在过度标示的数据，例如艺术和娱乐）、数据混合的规模法则（小模型搜索比例），最终结果50%一般知识，25%的数学和推理，17%的代码，以及8%的多语言。</li>
<li>数据退火：高质量数据退火处理，对小模型有效，大模型忽略不计（数据退火是一种在深度学习训练过程中动态调整数据分布或数据选择的方法）</li>
</ul>
</li>
<li>模型架构<ul>
<li>整体情况：相比llama2变动不大，性能提升主要是由于数据质量和多样性的提高以及训练规模的增加</li>
<li>GQA：分组查询注意力和8个键值头来提高推理速度，并在解码过程中减少键值缓存的大小</li>
<li>注意力掩码：使用注意力掩码来防止同一序列内不同文档之间的自注意力，在标准预训练中的影响有限，但在对非常长序列的持续预训练中很重要</li>
<li>token词汇表：128K tokens的词汇表，压缩率从3.17提高到3.94字符/token</li>
<li>RoPE：将RoPE基础频率超参数增加到500,000。这使我们能够更好地支持更长的上下文</li>
<li>参数搜索：不同参数模型的损失函数做曲线拟合<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-07.png" style="max-width:500px"></div></li>
</ul>
</li>
<li>基础设施、扩展和效率<ul>
<li>训练基础设置<ul>
<li>计算：16K个H100 GPU，每个GPU运行700W TDP(Thermal Design Power)，配备80GB HBM3，使用Meta的Grand Teton AI服务器平台</li>
<li>存储：Meta的通用分布式文件系统，7500台配备SSD的服务器中的240PB存储空间，并支持持续吞吐量2TB/s和峰值吞吐量7TB/s</li>
<li>网络：基于Arista 7800和Minipack2开放计算项目OCP机架交换机的RoCE网络，较小模型使用Infiniband网络，RoCE和Infiniband集群都在GPU之间利用400Gbps互连</li>
</ul>
</li>
<li>模型扩展的并行性<ul>
<li>4D并行性（四种不同类型的并行方法的组合来分片模型），这种方法有效地在许多GPU上分布计算，并确保每个GPU的模型参数、优化器状态、梯度和激活适合其HBM</li>
<li>张量并行性（TP）：模型的张量（如权重矩阵）分割到多个设备（如GPU）上进行并行计算，设备间需要通信</li>
<li>流水线并行性（PP）：将模型的不同层分配到不同的设备（如GPU）上，并在这些设备之间进行流水线式的计算，设备间不需要通信</li>
<li>上下文并行性（CP）：将不同的上下文（如不同的数据样本或不同的输入序列）分配到不同的设备（如GPU）上进行并行计算，设备间需要通信</li>
<li>数据并行性（DP）：将训练数据分割成多个子集，并将这些子集分配到不同的设备（如GPU）上进行并行计算<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-08.png" style="max-width:700px"></div></li>
</ul>
</li>
</ul>
</li>
<li>预训练<ul>
<li>初始预训练<ul>
<li>学习率：余弦学习率，峰值为8<em>10e-5，线性预热8000步，然后在120万训练步骤中衰减到8</em>10e-7</li>
<li>数量配置：最初使用4M（指总数据量，可取256）批量大小/长度为4,096的序列，预训练2.52亿tokens后加倍到8M批量大小/长度为8,192的序列，预训练了2.87T令牌后再次加倍到16M批量大小</li>
<li>这种训练配置非常稳定，损失尖峰很少，并且不需要干预来纠正模型训练发散</li>
<li>数据混合：在训练器件对预训练数据混合进行了几次调整，以提高模型在特定下游任务上的性能。上采样数学数据，预训练的后期增加了更多的最新数据，下采样质量较低的预训练数据子集</li>
</ul>
</li>
<li>长上下文预训练<ul>
<li>预训练的最后阶段对长序列进行训练，以支持高达128K令牌的上下文窗口，在最后逐步增加的原因是长序列计算复杂度随长度次方增长，在最后逐步提升能够提升训练效率（6个阶段）。</li>
</ul>
</li>
<li>退火<ul>
<li>在最后4000万个训练令牌的预训练期间，线性地将学习率退火到0，同时保持128K令牌的上下文长度</li>
</ul>
</li>
</ul>
</li>
<li>后训练处理<ul>
<li>建模框架：包括监督式微调（SFT）和直接偏好优化（DPO）<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-09.png" style="max-width:700px"></div><ul>
<li>奖励模型：与LLaMA2相同，用对比损失进行训练。</li>
<li>监督微调（SFT）：用拒绝采样的结果进行训练。</li>
<li>直接偏好优化（DPO）：摒弃强化学习的方式，直接用对比损失进行训练，方式和奖励模型类似，效果比强化学习要好。</li>
<li>迭代轮次：遵循LLaMA2的做法，总共进行六轮迭代训练。</li>
</ul>
</li>
<li>后训练数据<ul>
<li>偏好数据：四个等级（明显更好、更好、略好或稍微更好），还可以通过编辑修改最佳答案，编辑后的答案是最佳的。</li>
<li>SFT数据：人工收集的prompt，通过拒绝采样从模型中获取数据；针对特定能力的合成数据；少量人工策划的数据。</li>
<li>拒绝采样：根据prompt从模型中采样10-30个结果，并通过奖励模型选择最佳答案。</li>
<li>数据处理和质量控制：<ul>
<li>数据清洗：过度使用的表情、符号和短语。</li>
<li>数据修剪：基于模型的技术来移除低质量的训练样本，包括主题分类、质量评分、难度评分、语意去重。</li>
</ul>
</li>
</ul>
</li>
<li>应用能力<ul>
<li>代码：<ul>
<li>专家模型：额外训练一个专家模型，训练数据85%以上都是代码数据，在后续用于收集高质量的代码人工标注数据。</li>
<li>合成数据：使用LLaMA3和专家模型生成了许多低成本的合成代码数据。<ul>
<li>执行反馈：更复杂模型合成的数据对低复杂度模型有效，但对自己无效，甚至还会降低性能，因此需要引入真实的执行反馈。</li>
<li>编程语言翻译：将常用语言翻译成其他语言，来补充其他语言的数据。</li>
<li>回译：通过代码解释，再让模型根据解析复现代码，最后根据前后的结果进行SFT。</li>
</ul>
</li>
<li>在拒绝采样中使用提示词引导代码生成，提升可读性。</li>
<li>用执行情况和模型来过滤训练数据。</li>
</ul>
</li>
<li>多语言性<ul>
<li>专家模型：额外训练一个专家模型，训练数据90%以上都是非英语的多语言数据，并进行后训练调整，在后续被用于收集更高质量的非英语数据。</li>
<li>多语言数据收集：2.4%人工标注，44.2%其他NLP任务的数据，18.8%拒绝采样数据，34.6%翻译推理数据</li>
</ul>
</li>
<li>数学和推理<ul>
<li>问题：缺乏提示、缺乏真实思考过程链、中间步骤可能不正确、教授模型使用外部工具、训练和推理之间的差异</li>
<li>解决提示缺乏问题：从数学中获取相关的预训练数据，并将其转换为问答格式，然后可用于监督式微调</li>
<li>用逐步推理痕迹增强训练数据：使用Llama 3为一组提示生成逐步解决方案</li>
<li>过滤不正确的推理痕迹：训练结果和逐步奖励模型来过滤中间推理步骤是不正确的训练数据</li>
<li>交错代码和文本推理：提示Llama 3通过文本推理和相关Python代码的组合来解决推理问题</li>
<li>从反馈和错误中学习：为了模拟人类反馈，利用不正确的结果提示Llama 3产生正确的生成来进行错误纠正</li>
</ul>
</li>
<li>长文本<ul>
<li>最后的预训练阶段，将Llama 3的上下文长度从8K令牌扩展到128K令牌</li>
<li>合成数据：使用早期版本的Llama 3根据关键的长文本用例生成合成数据，包括问答、长文档的摘要以及对代码库的推理</li>
<li>将0.1%的合成生成的长文本数据与原始短文本数据混合，可以在短文本和长文本基准测试中优化性能</li>
</ul>
</li>
<li>工具使用<ul>
<li>工具调用：搜索引擎、Python解释器、数学计算引擎<ul>
<li>如果查询需要多次调用工具，模型可以编写逐步计划，按顺序调用工具，并在每次工具调用后进行推理</li>
</ul>
</li>
<li>实现方案：不同工具定义为Python对象，通过函数来实现调用</li>
<li>数据收集：人工标注和偏好数据，在消息级别进行注释以收集细粒度反馈</li>
<li>工具数据集：在某个checkpoint上进行微调，从单轮工具使用注释开始，然后转向对话中的工具使用，最后注释多步工具使用和数据分析</li>
<li>零样本工具使用数据：大量多样的部分合成数据集，包括单个/嵌套和并行函数调用、多轮函数调用</li>
</ul>
</li>
<li>事实性<ul>
<li>原则：训练后应该使模型“知道它所知道的”，即使模型生成与预训练数据中呈现的事实数据子集一致。</li>
<li>流程：从预训练数据中提取数据片段，用Llama 3生成事实性问题，同时采样Llama 3对上述问题的答案，再利用Llama 3进行评分，对于不正确的相应，让Llama 3拒绝回答。</li>
<li>总之就是探索模型的答案，然后根据答案设置边界，超过边界就让模型拒绝回答。</li>
</ul>
</li>
<li>可引导性<ul>
<li>可引导性是将模型的行动和结果引导至满足开发者和用户规范的能力</li>
<li>Llama 3专注于通过自然语言指令的系统提示来增强其可引导性，尤其是在响应长度、格式、语调和角色/个性方面</li>
<li>数据收集：在普通英语类别中收集可引导性偏好样本，然后通过人工对话进行评估。</li>
<li>建模；将这些数据在奖励建模、拒绝采样、SFT和DPO中应用</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>推理效率：流水线并行、FP8量化</li>
<li>视觉实验<ul>
<li>两个模型之间引入和训练一组交叉注意力层，将预训练的图像编码器和预训练的语言模型组合起来，用于大量图像-文本对<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-10.png" style="max-width:900px"></div></li>
<li>五个阶段训练的多模态模型：语言模型预训练，多模态编码器预训练，视觉适配器训练，模型微调，语音适配器训练</li>
<li>图像/视频数据：质量过滤，感知去重，重新采样，光学字符识别，安全措施</li>
<li>模型架构：<ul>
<li>图像编码器：标准的视觉transformer，多层特征提取，7680维表示</li>
<li>图像适配器：语言模型的每第四个自注意力层之后应用交叉注意力层</li>
<li>视频适配器：每个视频均匀采样最多64帧图片，编码后的视频帧通过时间聚合器进行聚合，在每第四个图像交叉注意力层之前添加额外的视频交叉注意力层</li>
</ul>
</li>
<li>模型扩展<ul>
<li>模型异构性：每个流水线阶段使用四个自注意力层和一个交叉注意力层</li>
<li>数据异构性：图像比相关文本有更多的token，交叉注意力层的计算比自注意力层的计算需要更多的时间和内存，在图像编码器中引入序列并行性，以便每个GPU处理大致相同数量的token</li>
<li>数值不稳定性：引入图像信息之后，bf16精度会导致数值不稳定，因此需要FP32精度来训练</li>
</ul>
</li>
<li>预训练<ul>
<li>图像：固定文本部分参数，训练图像编码器，学习率+训练样本退火</li>
<li>视频：添加了视频聚合器和交叉注意力层，其他部分参数固定，并在视频预训练数据上训练</li>
</ul>
</li>
<li>后训练<ul>
<li>SFT数据：图像（学术数据集、人类标注数据、合成数据），视频（学术数据集简单转换）</li>
<li>SFT训练配置：图像（预训练的模型进行初始化，语言参数训练中不变），视频（类似于图像进行初始化和训练，最后用视频数据进行微调）</li>
<li>偏好数据：人类标注、合成数据、拒绝采样（所有未被选择的生成都可以用作负面拒绝样本，并用作额外的偏好数据对）</li>
<li>奖励建模：在视觉SFT模型和语言RM的基础上训练一个视觉奖励模型，视觉编码器和交叉注意力层参数可训练，语言模型部分参数固定</li>
<li>直接偏好优化DPO：视觉DPO模型在每次微调迭代的人类评估中的表现始终优于其SFT版本</li>
</ul>
</li>
<li>拒绝采样：大多数可用的问答对只包含最终答案，缺少思维链解，使用拒绝采样为这些例子生成缺失的解释，增强模型的推理能力<ul>
<li>给定一个问题-答案对，通过使用不同的系统提示或温度对微调模型进行采样，生成多个带过程解释的答案</li>
<li>通过启发式或LLM裁判将生成的答案与真实答案进行比较，选择正确答案重新添加到微调数据混合中来重新训练模型，发现每个问题保留多个正确答案很有用</li>
<li>为确保数据的高质量，只选取了正确答案率较高的数据，并用奖励模型选择TopK的结果</li>
</ul>
</li>
<li>质量调整（QT）<ul>
<li>策划了一个小而高度精选的SFT数据集，所有样本都经过人类最好的模型重写和验证。</li>
<li>使用这些数据训练DPO模型以提高响应质量，将这个过程称为质量调整（QT）</li>
</ul>
</li>
</ul>
</li>
<li>语音实验<ul>
<li>结合一个编码器和一个适配器来处理语音信号，并用系统提示模型正在处理语音信号，以改善使用体验。还支持Llama 3用于自动语音识别（ASR）和自动语音翻译（AST），同时尝试了流式文本到语音（TTS）系统<br />
<div style="text-align: center"><img src="/wiki/attach/images/LLaMA-11.png" style="max-width:900px"></div></li>
<li>数据<ul>
<li>语音理解：预训练数据（多种语言的大约1500万小时的语音录音数据集），语音识别和翻译数据（ASR用23万小时的手动转录语音录音，AST用9万小时的翻译数据），口语对话数据</li>
<li>语音生成：用于训练文本规范化（TN）模型和韵律模型（PM）<ul>
<li>文本规范化数据包括5.5万个样本，涵盖了需要非平凡规范化的广泛符号类别（例如，数字、日期、时间）。每个样本是书面形式文本和相应的规范化口语形式文本的配对，以及执行规范化的手工TN规则的推断序列。</li>
<li>韵律模型数据包括从5万小时流式文本到语音（TTS）数据集中提取的语言学和韵律特征，这些是由专业配音演员在录音室环境中录制的配对文稿和音频。</li>
</ul>
</li>
</ul>
</li>
<li>模型架构<ul>
<li>语音理解：语音编码器和适配器<ul>
<li>语音编码器：<ul>
<li>一个具有10亿参数的Conformer模型，模型输入为80维的mel频谱图特征，首先经过一个步长为4的堆叠层处理，然后通过线性投影将帧长度减少到40毫秒，然后由24层Conformer层处理得到特征。</li>
<li>每个Conformer层具有1536的潜在维度，并包含两个Macron-net风格的前馈网络，其维度为4096，一个卷积模块，核大小为7，以及一个旋转注意力模块和24个注意力头。</li>
</ul>
</li>
<li>语音适配器：<ul>
<li>包含大约1亿参数，由一个卷积层、一个旋转Transformer层和一个线性层组成。卷积层的核大小为3，步长为2，旨在将语音帧长度减少到80毫秒。</li>
<li>Transformer层具有3072的潜在维度和一个4096维度的前馈网络，这在卷积下采样后进一步处理语音信息和上下文。最后，线性层将输出维度映射以匹配语言模型嵌入层。</li>
</ul>
</li>
</ul>
</li>
<li>语音生成：<ul>
<li>文本规范化：作为生成语音语义正确性的决定因素，文本规范化（TN）模块执行从书面形式文本到最终由下游组件口头表达的口语形式的上下文感知转换。例如，书面形式文本123根据语义上下文被读作基数词（一百二十三）或逐位拼写（一二三）</li>
<li>韵律建模：集成了一个仅解码器的基于Transformer的韵律模型（PM）），它将Llama 3嵌入向量作为额外的输入</li>
</ul>
</li>
<li>训练方法<ul>
<li>语音理解：语音预训练（自监督BEST-RQ算法预训练语音编码器） 和 监督微调（预训练的语音编码器和随机初始化的适配器在监督微调阶段进一步与Llama 3一起优化）两个阶段</li>
<li>语音生成：为了支持实时处理，韵律模型采用前瞻机制，考虑固定数量的未来音素和变化数量的未来标记，这确保了在处理传入文本时具有一致的前瞻机制</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2024 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2024-11-26 17:22:52</p>
      </span>
    </div>

    
    
  </body>
</html>