<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>Pattern Recognition and Machine Learning - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#09-Book">09-Book</a>&nbsp;&#187;&nbsp;Pattern Recognition and Machine Learning
    <span class="updated">Page Updated&nbsp;
      2021-07-01
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">Pattern Recognition and Machine Learning</div>

  <h2 id="_1">主要内容</h2>
<ul>
<li>
<p>1 绪论</p>
<ul>
<li>频率学家观点——统计机器学习（优化问题，参数是定值）<ul>
<li>似然函数是优化目标，最大似然估计；先验概率*似然概率是优化目标，最大后验概率；两者都是点估计，参数是定值。</li>
<li>定义模型，构造损失函数，求解最优化问题</li>
</ul>
</li>
<li>贝叶斯观点——概率图模型（积分问题，参数是分布）<ul>
<li>后验概率=先验概率*似然概率/归一化，自然包含先验概率信息</li>
<li>推断后验概率，均值、方差，积分问题，数值计算方法MCMC</li>
<li>有向（贝叶斯网络），无向（马尔可夫网络）；时间相关，离散（HMM隐马尔可夫），线性（卡尔曼滤波），非线性（Particle滤波）</li>
</ul>
</li>
<li>没有纯粹的贝叶斯观点或者频率学家观点，贝叶斯观点相比于频率学家观点的优势是自然包含了先验概率，其缺点也是依赖先验概率的选择；扔硬币10次都是正面推测11次是否正面的情况；本书着重强调贝叶斯观点。</li>
<li>概率论<ul>
<li>两个基本规则：加和规则（求边缘概率），乘积规则（求条件概率）</li>
<li>贝叶斯定理：通过两个基本规则计算所得，后验概率=先验概率*似然概率/归一化</li>
<li>概率密度函数，累计分布函数；p(y)dy=p(x)dx推导出概率密度转换公式</li>
<li>期望、方差和协方差；期望是指根据概率密度函数计算的预期均值，方差指一维空间的离散程度，协方差指多维空间的离散程度（方差是其子集）。</li>
<li>贝叶斯概率：频率学家和贝叶斯观点，详细说明见上文。</li>
<li>高斯分布：均值，方差，归一化；【最大似然估计】低估了分布的方差；【最大似然】与【最小化平方和误差】结果一致；加上先验后【最大后验】和【最小化正则平方和误差】结果一致。</li>
<li>贝叶斯曲线拟合：待补充</li>
</ul>
</li>
<li>模型选择：⾚池信息准则，贝叶斯信息准则（BIC）。</li>
<li>维度灾难：在⾼维空间中，⼀个球体的⼤部分体积都聚集在表⾯附近的薄球壳上。</li>
<li>决策论<ul>
<li>选择最大后验概率的结果</li>
<li>最小化错误分类概率</li>
<li>最小化期望损失</li>
<li>拒绝选项：置信度低时，拒绝识别样本</li>
<li>推断和决策：inference and decision<ul>
<li>生成式模型：对P（x，y）建模，然后计算后验概率，最后用决策论判定。<ul>
<li>需要求解的东西最多，但可以计算边缘概率p（x）（对于异常点检测有用）。</li>
</ul>
</li>
<li>判别式模型：直接推断后验概率P（y|x），之后用决策论判定。<ul>
<li>只是用于分类的话，直接求后验概率更简单。</li>
</ul>
</li>
<li>直接映射：根据x直接输出结果，结合推断和决策，很难在不考虑后验概率的情况下进行决策。</li>
<li>频率学派与贝叶斯学派是两种学派对于概率的看法不同而分的派系，参数是定值还是概率分布，两者之间没有必然联系。</li>
</ul>
</li>
<li>回归问题，与上述分类问题类似，有下列三个求解思路<ul>
<li>先对 p(x,t)进行建模，然后条件概率密度p(t|x)，最后加权积分得到条件期望。</li>
<li>推断 p(t|x)，再加权积分得到条件期望。</li>
<li>直接确定一个回归函数y(x)</li>
</ul>
</li>
</ul>
</li>
<li>信息论<ul>
<li>信息熵，最大熵离散分布是均匀分布，连续变量是⾼斯分布（限制一阶矩/二阶矩/归一化）；离散熵和微分熵。</li>
<li>相对熵（KL散度），凸函数，jensen不等式说明kl散度大于等于0，互信息。 </li>
</ul>
</li>
</ul>
</li>
<li>
<p>2 概率分布</p>
<ul>
<li>二元变量<ul>
<li>伯努利分布/二项分布，均值/方差/似然函数/最大似然估计，最大似然可能存在严重过拟合，故需要引入先验概率。</li>
<li>后验分布与先验分布有着同样的函数形式，称为共轭性。</li>
<li>Beta分布作为先验概率分布，对于伯努利分布具有共轭性，后验分布同样也是Beta分布；根据观测到的数据，后验分布可以自然的更新。</li>
<li>因此如果选择贝叶斯观点，那么学习过程中的顺序（sequential）⽅法可以⾃然⽽然地得出，它与先验和似然函数的选择⽆关，只取决于数据独⽴同分布的假设。</li>
</ul>
</li>
<li>多项式变量<ul>
<li>多项式分布，狄利克雷是多项式分布的共轭先验。</li>
</ul>
</li>
<li>高斯分布<ul>
<li>对于⼀个⼀元实值向量，使熵取得最⼤值的是⾼斯分布（限制一阶矩/二阶矩/归一化），这个性质对于多元⾼斯也成⽴。</li>
<li>拉普拉斯提出的中⼼极限定理（central limit theorem）：对于某些温和的情况，⼀组随机变量之和（当然也是随机变量）的概率分布随着和式中项的数量的增加⽽逐渐趋向⾼斯分布。</li>
<li>将高斯分布的协方差矩阵转化为特征向量求和形式，可以将高斯分布的形式转化为D个独⽴⼀元⾼斯分布的乘积。</li>
<li>条件高斯分布/边缘高斯分布<ul>
<li>多元⾼斯分布的⼀个重要性质是，如果两组变量是联合⾼斯分布，那么以⼀组变量为条件，另⼀组变量同样是⾼斯分布。类似地，任何⼀个变量的边缘分布也是⾼斯分布。</li>
</ul>
</li>
<li>高斯变量的贝叶斯定理<ul>
<li>给定x的⼀个边缘⾼斯分布，以及在给定x的条件下y的条件⾼斯分布，可以得到y的边缘高斯分布以及给定y的条件下x的条件高斯分布。</li>
</ul>
</li>
<li>高斯分布的最大似然估计<ul>
<li>最大似然估计结果的均值是无偏估计，方差是偏小的。</li>
</ul>
</li>
<li>顺序估计<ul>
<li>将最大似然估计看作样本粒度的计算，观测到新样本之后进行更新。</li>
</ul>
</li>
<li>⾼斯分布的贝叶斯推断<ul>
<li>后验概率正比于先验函数*似然函数</li>
<li>高斯函数作为先验分布，是高斯分布似然函数的共轭分布，后验还是高斯分布似然函数的形式。</li>
<li>若观测样本数量为0，则推断均值为先验分布均值；若观测样本数量接近无穷，则推断均值为最大似然估计均值；因此从顺序估计的角度来看，贝叶斯观点是十分自然的。</li>
<li>推断方差也是类似的思路。</li>
</ul>
</li>
<li>学⽣t分布<ul>
<li>无限个同均值不同方差的高斯分布求和得到，即无限混合高斯模型。</li>
<li>这个分布通常有着⽐⾼斯分布更长的“尾巴”，这给出了t分布的⼀个重要性质：鲁棒性（robustness），意思是对于数据集⾥的⼏个离群点outlier的出现，t分布不会像⾼斯分布那样敏感。</li>
</ul>
</li>
<li>周期变量<ul>
<li>⾼斯分布在实际应⽤中⾮常重要，但有些情况下使⽤⾼斯分布建模并不合适，⼀个重要的情况是周期变量。</li>
<li>虑⾼斯分布对于周期变量的⼀个推⼴（von Mises分布）或者环形正态分布（circular normal）。</li>
</ul>
</li>
<li>混合⾼斯模型<ul>
<li>通过将更基本的概率分布（例如⾼斯分布）进⾏线性组合得到混合模型。</li>
<li>高斯混合模型利用EM算法求解，在第9章中会详细讨论。</li>
</ul>
</li>
</ul>
</li>
<li>指数族分布<ul>
<li>伯努利分布，多项式分布，高斯分布。</li>
<li>最⼤似然与充分统计量<ul>
<li>指数族分布的似然函数于最大似然估计。</li>
<li>我们不需要存储整个数据集本⾝，只需要存储充分统计量的值即可描述该分布的某个估计。</li>
</ul>
</li>
<li>共轭先验<ul>
<li>对于指数族分布的任何成员，都存在⼀个共轭先验。</li>
</ul>
</li>
<li>⽆信息先验<ul>
<li>在某些概率推断的应⽤中，我们可能有⼀些先验知识，可以⽅便地通过先验概率分布来表达；但是，在许多情形下我们可能对分布应该具有的形式⼏乎完全不知道。</li>
<li>这时我们可以寻找⼀种形式的先验分布，被称为⽆信息先验（noninformative prior）；这种先验分布的⽬的是尽量对后验分布产⽣尽可能⼩的影响，也被称为“让数据⾃⼰说话”。</li>
</ul>
</li>
</ul>
</li>
<li>⾮参数化⽅法<ul>
<li>本章上述的概率分布都有具体的函数形式，并且由少量的参数控制，这些参数的值可以由数据集确定，这被称为概率密度建模的参数化（parametric）⽅法。</li>
<li>参数化方法的⼀个重要局限性是选择的概率密度可能对于⽣成数据来说，是⼀个很差的模型，从⽽会导致相当差的预测表现。</li>
<li>本节考虑⼀些⾮参数化（nonparametric）⽅法进⾏概率密度估计，这种⽅法对概率分布的形式进⾏了很少的假设。</li>
<li>假设观测服从D维空间的某个未知的概率密度分布p(x)=K/NV，K为落入目标空间中的样本数量，N为样本总数，V为空间体积。</li>
<li>固定K来确定V是K近邻法；固定V来确定K则是核方法；在极限N趋近于无穷的情况下，如果V随着N⽽合适地收缩，并且K随着N增⼤，那么可以证明K近邻概率密度估计和核⽅法概率密度估计都会收敛到真实的概率密度。</li>
<li>核密度估计<ul>
<li>为了确定概率密度p(x)，把区域R取成以x为中⼼的⼩超⽴⽅体（确定V），统计落在这个区域内的数据点的数量K（确定K），用核函数来表示是否在区域内。</li>
<li>常用的核函数有高斯核函数。</li>
</ul>
</li>
<li>近邻⽅法<ul>
<li>为了确定概率密度p(x)，以x为中⼼的⼩球体，将球体的半径逐渐变长，直到它精确地包含K个数据点。</li>
<li>常见的方法是K近邻法。</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>3 线性回归</p>
<ul>
<li>线性基函数模型</li>
<li>最大似然与最小平方（奇异矩阵，参数很大，通过加正则项来解决）</li>
<li>偏置方差</li>
<li>贝叶斯线性回归（先验分布、似然分布、后验分布和预测分布）</li>
<li>最小二乘与几何意义<ul>
<li>最小二乘推导</li>
<li>两种几何解释<ul>
<li>最小化误差距离</li>
<li>在x构成的空间内找最接近y的函数（X和Y-aX垂直）</li>
</ul>
</li>
</ul>
</li>
<li>最大似然角度推导</li>
<li>正则化，频率/贝叶斯 两个角度的推导</li>
<li>LSE（最小二乘）== MLE（高斯噪声）</li>
<li>Regularized LES == MAP（贝叶斯回归，高斯先验，高斯噪声）</li>
</ul>
</li>
<li>
<p>4 线性分类</p>
<ul>
<li>频率派：统计机器学习，贝叶斯派：概率图模型</li>
<li>线性<ul>
<li>属性非线性-多项式回归</li>
<li>全局非线性-0/1分类</li>
<li>系数非线性-神经网络</li>
</ul>
</li>
<li>全局性<ul>
<li>决策树</li>
<li>线性样条回归</li>
</ul>
</li>
<li>数据加工<ul>
<li>PCA</li>
<li>流形</li>
</ul>
</li>
<li>线性分类<ul>
<li>硬分类，0或1<ul>
<li>svm</li>
<li>感知机</li>
<li>线性判别分析，fisher</li>
</ul>
</li>
</ul>
</li>
<li>软分类，0～1取值<ul>
<li>生成式，高斯判别分析（连续），朴素贝叶斯（离散）</li>
<li>判别式，lr</li>
</ul>
</li>
<li>二分类与多分类</li>
<li>分类中的最小平方</li>
<li>fisher线性判别函数（确定投影方向），与最小平方一致</li>
<li>感知器算法</li>
</ul>
</li>
<li>
<p>5 神经网络</p>
</li>
<li>
<p>6 核方法</p>
</li>
<li>
<p>降维</p>
<ul>
<li>直接降维（特征选择）</li>
<li>线性降维，PCA，MDS</li>
<li>非线性降维，流形（LLE局部线性嵌入，soimap）</li>
</ul>
</li>
<li>
<p>7 SVM</p>
<ul>
<li>二次凸优化问题</li>
<li>约束优化问题——对偶问题</li>
</ul>
</li>
<li>
<p>8 概率图模型</p>
<ul>
<li>概率图结构，tail-to-tail/tail-to-head/head-to-head<ul>
<li>tail-to-tail/tail-to-head，节点已知则路径两侧独立。</li>
<li>head-to-head，节点与子节点未知则路径两侧独立。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>9 EM</p>
</li>
<li>
<p>10 近似推断</p>
</li>
<li>
<p>11 采样方法</p>
<ul>
<li>精确推断不可行，故使用采样方法；虽然我们感兴趣的是⾮观测变量上的后验概率分布本⾝，但是在⼤部分情况下后验概率分布的主要⽤途是计算期望，例如做预测的场景。</li>
<li>概率密度转换公式，利用简单分布采样能力对复杂分布进行采样。</li>
<li>拒绝采样：已知概率密度相对值，但归一化系数难以计算的情况；为了降低拒绝概率，proposal分布应该尽可能的接近目标分布；proposal分布选择难度高，可以利用切线来构造；拒绝采样不适合高维空间。</li>
<li>重要性采样：通过一个相似的proposal简单分布采样来近似目标分布，其中对不同的采样值进行了重要性权重的修正。</li>
<li>蒙特卡罗EM算法：EM中E步骤无法解析计算的模型用采样方法来近似。</li>
<li>MCMC<ul>
<li>蒙特卡洛方法：通过采样样本来近似代表所有样本，当采样样本数量足够大时，采样样本就可以近似全部样本；蒙特卡洛方法求圆周率。</li>
<li>随机过程：随时间变化的一系列随机变量。</li>
<li>马尔可夫性质：t+1状态的取值只取决于t状态。</li>
<li>马尔可夫过程：具有马尔可夫性质的随机过程成为马尔可夫过程，也可定义为连续指数集的马尔可夫链。</li>
<li>平稳性分布性质：系统在随机时间所在状态的概率是确定的，即平稳分布，也成为静态分布。</li>
<li>马尔可夫链蒙特卡洛方法（MCMC）：是一类以期望分布为平稳分布的马尔可夫链为基础，对概率分布进行抽样的算法。</li>
<li>整体思路：为了对后验分布进行采样，构建一个马尔可夫链（利用似然比来作为接受率，然后用蒙特卡洛方法进行采样），其稳态的分布与后验分布一致。</li>
<li>吉布斯采样：用于在难以直接采样时从某一多变量概率分布中近似抽取样本序列。</li>
</ul>
</li>
</ul>
</li>
<li>
<p>12 连续潜在变量</p>
</li>
<li>
<p>13 顺序数据</p>
</li>
<li>
<p>14 组合模型</p>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2024 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2024-03-25 11:08:50</p>
      </span>
    </div>

    
    
  </body>
</html>