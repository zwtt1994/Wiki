<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2018-Graph Convolutional Neural Networks for Web-Scale Recommender Systems - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#03-GNN">03-GNN</a>&nbsp;&#187;&nbsp;2018-Graph Convolutional Neural Networks for Web-Scale Recommender Systems
    <span class="updated">Page Updated&nbsp;
      2020-07-05
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2018-Graph Convolutional Neural Networks for Web-Scale Recommender Systems</div>

  <h2 id="_1">总结</h2>
<ul>
<li>在大规模数据中落地的GCN应用，本质上还是GraphSAGE的思想，在具体训练中做了一些加速操作。</li>
</ul>
<h2 id="_2">主要内容</h2>
<div style="text-align: center"><img src="/wiki/attach/images/GCN-RS-00.png" style="max-width:700px"></div>

<ul>
<li>思路和GraphSAGE一样，是将邻居节点的信息进行聚合。<br />
<div style="text-align: center"><img src="/wiki/attach/images/GCN-RS-01.png" style="max-width:500px"></div></li>
<li>聚合方法本质是GraphSAGE中提到的pooling方法，先对邻居节点进行非线性变换，再利用加权pooling的方式统一维度，与本节点的向量concat之后，再进行一次非线性变换，最后做归一化。</li>
<li>邻居节点的选择是先通过随机游走生成领域，并将游走中对节点的访问次数的归一化值来定义邻居的重要性，并取前K个。</li>
<li>每个节点（mini-batch）邻居搜索和卷积计算整体流程如下。<br />
<div style="text-align: center"><img src="/wiki/attach/images/GCN-RS-02.png" style="max-width:500px"></div></li>
<li>模型训练细节<ul>
<li>目的是生成pins的embedding表示，数据是一个无向图数据，如果两个pin之间有board则认为是相关的。</li>
<li>hinge损失函数，利用了相关向量内积与非相关向量内积的差，负样本通过负采样得到。</li>
<li>对mini-batch的计算结构进行了GPU分布式操作，训练时的batch size范围是512-4096。</li>
<li>训练时每次只加载对应mini-batch的数据内存。</li>
<li>负采样方面，每个mini-batch用一组负样本，并对每组相关的pins加入了"hard"负样本，这些样本和当前mini-batch节点不相关，但与改正样本组中另一个节点相关。</li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2021 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2021-01-10 21:46:03</p>
      </span>
    </div>

    
    
  </body>
</html>