<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2017-Inductive Representation Learning on Large Graphs - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#03-GNN">03-GNN</a>&nbsp;&#187;&nbsp;2017-Inductive Representation Learning on Large Graphs
    <span class="updated">Page Updated&nbsp;
      2020-07-12
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2017-Inductive Representation Learning on Large Graphs</div>

  <h2 id="_1">总结</h2>
<ul>
<li>提出了GraphSAGE算法，本质上是基于组合的空间GCN，通过叠加邻居节点的信息来更新节点的表示，目的是得到更高阶的领域信息。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>常规的图神经网络都是对固定的图结构进行学习，泛化能力较差；本文的主要思想是直接学习节点的表示方法，而不受图结构的影响。<br />
    <div style="text-align: center"><img src="/wiki/attach/images/GraphSAGE-01.png" style="max-width:600px"></div></p>
</li>
<li>
<p>GraphSAGE算法本质上是通过一个"Aggregate函数"来表示目标节点的邻居信息，并将该目标节点与邻居信息拼接后计算得到下一层的输出，可以看到整个算法是逐层逐节点计算的。</p>
</li>
<li>
<p>邻居节点选择</p>
<ul>
<li>如果邻居节点较多可以进行采样，由于算法是逐层计算的，所以也采用基于层的采样方式。</li>
<li>文中提到了一些超参的选择，在层数取2，邻居采样为20个左右时，效果已经不错了。</li>
</ul>
</li>
<li>
<p>Aggregate函数</p>
<ul>
<li>由于图结构中节点的邻居是无序的，所以聚合函数最好对输入顺序是不敏感的（或者说输出不随输入顺序改变），并且具有较好的表示能力。</li>
<li>Mean aggregator：均值聚合，将所有邻居向量取均值。</li>
<li>LSTM aggregator：LSTM聚合，由于LSTM对顺序敏感，所以在输入之前对邻居节点进行乱序。</li>
<li>Pooling aggregator：Pooling聚合，对邻居节点进行非线性变换之后，取向量max/mean操作。</li>
</ul>
</li>
<li>
<p>损失函数</p>
<ul>
<li>Graph embedding，在无监督学习下，由于GraphSAGE是由邻居聚合的形式前向计算的，会导致间隔较远的节点embedding相差较大，所以负样本的采样方式是在正样本节点附近进行随机游走，一定程度上解决了上述问题。</li>
<li>有监督任务按正常的损失函数定义即可。</li>
</ul>
</li>
<li>
<p>实验验证了GraphSAGE的效果<br />
    <div style="text-align: center"><img src="/wiki/attach/images/GraphSAGE-02.png" style="max-width:600px"></div></p>
<ul>
<li>Pooling或LSTM效果较好，LSTM的效果比较出人意料。</li>
<li>有监督结果好于无监督，但无监督效果也不差，所以可以在特定场景下作为预训练。</li>
<li>训练时间并DeepWalk长，但预测时间更短（未知节点可以直接计算）。</li>
<li>在邻居数量和训练时间之间做了权衡。</li>
<li>层数取2层相比与1层有10～15%的提升，但超过3层则进一步的提升小于5%，预测计算量则成倍增长（逐节点逐层）。</li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2023 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2023-12-15 17:12:30</p>
      </span>
    </div>

    
    
  </body>
</html>