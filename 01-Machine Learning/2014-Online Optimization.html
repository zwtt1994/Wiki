<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2014-Online Optimization  - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#01-Machine Learning">01-Machine Learning</a>&nbsp;&#187;&nbsp;2014-Online Optimization 
    <span class="updated">Page Updated&nbsp;
      2020-06-20
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2014-Online Optimization </div>

  <h2 id="_1">总结</h2>
<ul>
<li>为了在在线学习中利用L1正则得到参数稀疏的效果，提出了一系列辅助产生参数稀疏的策略。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>L1正则化由于次梯度的原因会对特征产生稀疏的情况，稀疏的好处就是进行特征选择，简化模型复杂度，提高训练效率。但在batch的模式下，单batch的SGD优化往往由于随机性而达不到稀疏的效果，但这在离线训练中可以通过所有样本以及多个epoch的方式来解决，因为用的只是最后训练完的模型。</li>
<li>在线学习中一般也是利用batch的形式来训练，但线上使用的就是单batch训练完之后的模型，此时就无法达到理想中L1的稀疏效果。</li>
<li>
<p>为了在在线学习中引入稀疏，提出了下面几个方法</p>
<ul>
<li>简单截断，每训练T次才做一次截断（避免随机性导致的错误截断）,在训练中对参数小于某个阈值的直接截断为0，简单粗暴。</li>
<li>
<p>Truncated Gradient（TG），与简单截断类似，是指在其中加上了对参数的微调。<br />
<div style="text-align: center"><img src="/wiki/attach/images/onlinTrain-01.png" style="max-width:500px"></div></p>
</li>
<li>
<p>Forward-Backward Splitting（FOBOS），前向后向切分，本质上是指当样本训练中某次更新后的参数绝对值不够大时令其为0，和TG其实是差不多的，区别是在全域上对参数做微调。<br />
<div style="text-align: center"><img src="/wiki/attach/images/onlinTrain-02.png" style="max-width:500px"></div></p>
</li>
<li>
<p>Regularized Dual Averaging（RDA），正则对偶平均，本质上是当某个参数维度累计梯度小某个值时令其为0。<br />
<div style="text-align: center"><img src="/wiki/attach/images/onlinTrain-03.png" style="max-width:500px"></div></p>
</li>
<li>
<p>FOBOS是batch维度做调整，训练精度较高，RDA则更容易产生稀疏性，所以结合上述两者提出了Follow the Regularized Leader（FTRL）。<br />
<div style="text-align: center"><img src="/wiki/attach/images/onlinTrain-04.png" style="max-width:500px"></div></p>
</li>
<li>
<p>FTRL和FPOBOS/RDA的型式基本一致，区别就是在于zi为历史梯度与权值差的累计，同时考虑了"历史累计"和"梯度与权值的差"。<br />
<div style="text-align: center"><img src="/wiki/attach/images/onlinTrain-05.png" style="max-width:200px"></div></p>
</li>
<li>
<p>此外，上述涉及到的学习率一般是随时间下降的序列，在FTRL中定义如下。<br />
<div style="text-align: center"><img src="/wiki/attach/images/onlinTrain-06.png" style="max-width:300px"></div></p>
</li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2021 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2021-07-04 18:21:29</p>
      </span>
    </div>

    
    
  </body>
</html>