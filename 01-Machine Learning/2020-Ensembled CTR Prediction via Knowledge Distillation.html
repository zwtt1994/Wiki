<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2020-Ensembled CTR Prediction via Knowledge Distillation - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#01-Machine Learning">01-Machine Learning</a>&nbsp;&#187;&nbsp;2020-Ensembled CTR Prediction via Knowledge Distillation
    <span class="updated">Page Updated&nbsp;
      2021-01-30
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2020-Ensembled CTR Prediction via Knowledge Distillation</div>

  <h2 id="_1">总结</h2>
<ul>
<li>知识蒸馏是在基本保留模型效果下，减小模型复杂度。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>目前推荐系统里的模型越来越复杂，虽然能够带来一定增益，但同时也对预估性能带来了很大的挑战；知识蒸馏则能够基本保持效果的同时降低模型复杂度。</p>
</li>
<li>
<p>"机器学习最根本的目的在于训练出在某个问题上泛化能力强的模型"，知识蒸馏的学习模式中，s模型学到的信息比传统的学习方式更丰富（因为学习的是个分布），这提高了负样本的权重。</p>
</li>
<li>
<p>单teacher网络结构如下：<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-01.png" style="max-width:500px"></div></p>
<ul>
<li>t模型可以预先训练完成，也可以和s模型一起训练；</li>
<li>损失函数一般包括两部分，s模型常规的损失函数，以及s和t模型的kd损失；<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-03.png" style="max-width:300px"></div></li>
<li>上述两个损失函数占比的超参可以控制s模型从真实label和t模型学习的比重；</li>
<li>kd损失可以用交叉熵，也可以用平方差；交叉熵中的tau可以控制模型学习分布的陡峭程度。<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-04.png" style="max-width:300px"></div><br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-05.png" style="max-width:300px"></div></li>
</ul>
</li>
<li>
<p>多teacher网络结构如下：<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-02.png" style="max-width:500px"></div></p>
<ul>
<li>多t模型可以看作是多模型融合下的单teacher蒸馏；</li>
<li>多t模型同样可以预先训练完成，也可以和s模型一起训练；</li>
<li>多t模型最终按attention方式融合，具体方式可以根据具体场景设计。</li>
</ul>
</li>
<li>
<p>一些实验结果如下：</p>
<ul>
<li>不同的训练设置；<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-06.png" style="max-width:500px"></div></li>
<li>不同的st组合；<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-07.png" style="max-width:800px"></div></li>
<li>不同的模型融合；<br />
<div style="text-align: center"><img src="/wiki/attach/images/kd-08.png" style="max-width:500px"></div></li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2023 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2023-12-15 17:00:58</p>
      </span>
    </div>

    
    
  </body>
</html>