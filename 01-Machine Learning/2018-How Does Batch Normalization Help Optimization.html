<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2018-How Does Batch Normalization Help Optimization - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#01-Machine Learning">01-Machine Learning</a>&nbsp;&#187;&nbsp;2018-How Does Batch Normalization Help Optimization
    <span class="updated">Page Updated&nbsp;
      2020-11-15
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2018-How Does Batch Normalization Help Optimization</div>

  <h2 id="_1">总结</h2>
<ul>
<li>论文利用实验得出BN与ICS无关，并且BN起作用的原因是因为它使得损失函数更加光滑。</li>
<li>其本质还是BN对网络输出进行了正则限制，规避了网络输出变化不可控的情况，同时可训练的shift&amp;scale也能够保持模型的表达能力，使得模型自适应的权衡正则与表达能力。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>internal covariate shif（ICS）：由于参数更新导致的隐藏层输出分布的变化。</p>
</li>
<li>
<p>为了提高参数分布的稳定性，可以尽量保持参数分布为标准正态分布；但由于每个mini-batch的数据分布不一致，且参数分布会随着训练调整，所以为了维持模型的表达能力，设置了可训练的归一化参数。</p>
</li>
<li>
<p>论文提出了两个问题，并做了实验验证。</p>
<ul>
<li>Batch normalization是否与ICS有关？在BN后面人为加上了shift噪声，发现和不加的效果差不多，所以论文认为BN和ICS没有关系。<br />
<div style="text-align: center"><img src="/wiki/attach/images/How-BN-01.png" style="max-width:600px"></div></li>
<li>Batch normalization是否会消除或者减弱ICS？通过量化隐藏层梯度在前一层更新参数的差异来近似评估ICS，发现BN并不能减少ICS，反而还有增加。<br />
<div style="text-align: center"><img src="/wiki/attach/images/How-BN-02.png" style="max-width:800px"></div></li>
</ul>
</li>
<li>
<p>论文认为BN起作用的原因是使得损失函数更加光滑，例如b-smoothness，即斜率绝对值不超过某个定值。<br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-03.png" style="max-width:800px"></div></p>
<ul>
<li>更光滑的损失函数更容易训练，Visualizing the Loss Landscape of Neural Nets。</li>
<li>附录中提到BN的作用与ResNet类似。</li>
</ul>
</li>
<li>
<p>由于平滑损失函数能够带来作用，论文探究了几种L-p归一化的方式，发现虽然ICS比较严重，但训练效果都不错。</p>
</li>
<li>
<p>对上述实验结果进行了理论推导分析，得出了几个结论；加入了BN之后：</p>
<ul>
<li>损失函数对于激活输出的梯度幅值更小。</li>
<li>损失函数对于激活输出的二阶导幅值也更小，即使得损失函数更加平滑。</li>
<li>损失函数对于权重的梯度幅值也更小。</li>
<li>权重初始值和最优值之间的距离也更接近。</li>
</ul>
</li>
<li>
<p>附录里的一些实验结果<br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-04.png" style="max-width:800px"></div><br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-05.png" style="max-width:800px"></div><br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-06.png" style="max-width:800px"></div><br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-07.png" style="max-width:800px"></div><br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-08.png" style="max-width:800px"></div><br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-09.png" style="max-width:800px"></div><br />
    <div style="text-align: center"><img src="/wiki/attach/images/How-BN-10.png" style="max-width:800px"></div></p>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2024 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2024-10-08 16:42:26</p>
      </span>
    </div>

    
    
  </body>
</html>