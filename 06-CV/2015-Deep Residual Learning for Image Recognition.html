<!DOCTYPE HTML>
<html>
  <head>
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/style.css">
    <link rel="Stylesheet" type="text/css" href="/wiki/static/css/tango.css">
    <link rel="shortcut icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="icon" href="/wiki/favicon.ico" type="image/x-icon">
    <link rel="alternate" type="application/atom+xml" href="atom.xml" title="Atom feed">
    <title>2015-Deep Residual Learning for Image Recognition - zwt的个人wiki</title>
    <meta name="keywords" content="Memory."/>
    <meta name="description" content="Wiki."/>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  </head>

  <body>
  <div id="container" style = "width: 65em">
      
<div id="header">
  <div class="post-nav"><a href="/wiki/">Home</a>&nbsp;&#187;&nbsp;<a href="/wiki/#06-CV">06-CV</a>&nbsp;&#187;&nbsp;2015-Deep Residual Learning for Image Recognition
    <span class="updated">Page Updated&nbsp;
      2020-11-15
    </span></div>
</div>
<div class="clearfix"></div>

<div class="page_title">2015-Deep Residual Learning for Image Recognition</div>

  <h2 id="_1">总结</h2>
<ul>
<li>提出了ResNet结构，使得神经网络能够学习到恒等变换，解决深层神经网络的训练问题。</li>
</ul>
<h2 id="_2">主要内容</h2>
<ul>
<li>
<p>理论上网络深度越深，模型表达能力越强，但梯度消失/爆炸问题对深度模型训练产生了阻碍。应对上述问题常见的有两种方式，参数初始化（Xavier）、中间层参数规范化（BN）。</p>
</li>
<li>
<p>当较深的网络能够收敛时，发现存在网络越深效果反而更差的现象（why？），并且这种现象并不是过拟合导致的，因为训练集的误差也变大了。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Res-01.png" style="max-width:400px"></div></p>
</li>
<li>
<p>如果深层网络可以学习到恒等变换，那么至少他的效果不会比只有浅层网络更差，所以论文推测mlp很难学习到恒等变换。</p>
</li>
<li>
<p>本文引入ResNet结构，使得网络能够较为容易的学到恒等变换。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Res-02.png" style="max-width:400px"></div></p>
</li>
<li>
<p>实验表明ResNet结构可以训练更深的网络，得到更好的结果。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Res-03.png" style="max-width:400px"></div></p>
</li>
<li>
<p>对比了三种短链接，效果依次提升：</p>
<ul>
<li>直接短接，size不够用0补充；</li>
<li>直接短接，size不够用投影变换；</li>
<li>每个短接都是一个投影变换。</li>
</ul>
</li>
<li>
<p>作者在下一篇论文Identity mapping in Deep Residual Networks深入探讨了res-net结构，并做了优化。</p>
<ul>
<li>去掉了res-net输出的relu激活函数，使得恒等变换更为直接。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Res-04.png" style="max-width:400px"></div></li>
<li>由于恒等变换短接的存在，基本不可能出现梯度消失的情况。</li>
<li>实验对比了各种短接结构，结果证明了恒等变换是最好的。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Res-05.png" style="max-width:400px"></div></li>
<li>实验对比了激活函数的影响，得到了一个效果最好的结构。<br />
<div style="text-align: center"><img src="/wiki/attach/images/Res-06.png" style="max-width:400px"></div></li>
</ul>
</li>
</ul>
    </div>
    <div id="footer">
      <span>
        <p>Copyright © 2020 zwt.
        Powered by <a href="http://simiki.org/" target="_blank">Simiki</a>.</p>
        <p>Site Generated 2020-11-15 21:19:29</p>
      </span>
    </div>

    
    
  </body>
</html>