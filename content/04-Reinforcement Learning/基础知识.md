---
title: "强化学习基础知识"
layout: page
date: 2024-10-04
---

## 主要内容

- 核心思想
    - 监督学习需要ground truth，但这个真理数据在很多情况下很难收集，然而判断哪个数据更好相对比较简单，强化学习就是基于该思路产生。

- 基本概念
    - 主体（agent）、环境（environment）、状态（state）、奖励（reward）、动作（action）
    - 基本学习框架如下，Agent感知环境，确定当前的state，根据policy采取action，获取reward，目标输出policy
<div style="text-align: center"><img src="/wiki/attach/images/RL-01.png" style="max-width:600px"></div>
    - 强化学习包括两大类方法，基于策略的(Policy-Based，建模输出action)方法和基于价值的(Value-Based，间接建模价值value)方法

- Policy-Based
    - 基于策略的方法，根据状态直接建模动作的分布
    - Policy Gradient，假设策略是一个参数化的随机策略
<div style="text-align: center"><img src="/wiki/attach/images/RL-02.png" style="max-width:600px"></div>
    - 策略梯度定理（Policy Gradient Theory）：目标函数对参数的梯度不包含环境信息
<div style="text-align: center"><img src="/wiki/attach/images/RL-03.png" style="max-width:400px"></div>
    - Reinfoce算法：一种MC更新方法，每完成一个episode（整个流程）才进行算法的更新，它存在较大的估计方差(variance)
<div style="text-align: center"><img src="/wiki/attach/images/RL-04.png" style="max-width:600px"></div>
    - Actor-Critic算法：TD更新方法，结合了策略梯度和基于价值的方法，解决了Reinfoce算法大方差的问题
<div style="text-align: center"><img src="/wiki/attach/images/RL-05.png" style="max-width:600px"></div>
        - 可以看到公式里把一个Episode的Reward替换为Critic的价值函数，价值估计更加准确
<div style="text-align: center"><img src="/wiki/attach/images/RL-06.png" style="max-width:600px"></div>
    - Advantage Actor-Critic：引入优势（Advantage）函数来减少估计的方差，通常定义为Q(s,a)-V(s)的差，近似为V(s(t+1))-V(s(t))
    - PPO（Proximal Policy Optimization ）：解决A2C算法不稳定的问题
        - 基本思想：希望更新参数时不要引起策略的太大变化
<div style="text-align: center"><img src="/wiki/attach/images/RL-07.png" style="max-width:600px"></div>

- Value-Based
    - 基于价值的方法，学习一个价值函数，包括状态价值函数 或者 状态-动作价值函数
    - 状态价值函数：Agent处于状态state，它期望可以获得的累计奖励
    - 状态-动作价值函数：Agent处于状态state，并且采取动作action之后可以获得的累计奖励
   

