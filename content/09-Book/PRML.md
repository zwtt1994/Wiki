---
title: "Pattern Recognition and Machine Learning"
layout: page
date: 2021-07-01
---

## 主要内容

- 1 绪论
    - 频率学家观点——统计机器学习（优化问题，参数是定值）
        - 似然函数是优化目标，最大似然估计；先验概率*似然概率是优化目标，最大后验概率；两者都是点估计，参数是定值。
        - 定义模型，构造损失函数，求解最优化问题
    - 贝叶斯观点——概率图模型（积分问题，参数是分布）
        - 后验概率=先验概率*似然概率/归一化，自然包含先验概率信息
        - 推断后验概率，均值、方差，积分问题，数值计算方法MCMC
        - 有向（贝叶斯网络），无向（马尔可夫网络）；时间相关，离散（HMM隐马尔可夫），线性（卡尔曼滤波），非线性（Particle滤波）
    - 没有纯粹的贝叶斯观点或者频率学家观点，贝叶斯观点相比于频率学家观点的优势是自然包含了先验概率，其缺点也是依赖先验概率的选择；扔硬币10次都是正面推测11次是否正面的情况；本书着重强调贝叶斯观点。
    - 概率论
        - 两个基本规则：加和规则（求边缘概率），乘积规则（求条件概率）
        - 贝叶斯定理：通过两个基本规则计算所得，后验概率=先验概率*似然概率/归一化
        - 概率密度函数，累计分布函数；p(y)dy=p(x)dx推导出概率密度转换公式
        - 期望、方差和协方差；期望是指根据概率密度函数计算的预期均值，方差指一维空间的离散程度，协方差指多维空间的离散程度（方差是其子集）。
        - 贝叶斯概率：频率学家和贝叶斯观点，详细说明见上文。
        - 高斯分布：均值，方差，归一化；【最大似然估计】低估了分布的方差；【最大似然】与【最小化平方和误差】结果一致；加上先验后【最大后验】和【最小化正则平方和误差】结果一致。
        - 贝叶斯曲线拟合：待补充
    - 模型选择：⾚池信息准则，贝叶斯信息准则（BIC）。
    - 维度灾难：在⾼维空间中，⼀个球体的⼤部分体积都聚集在表⾯附近的薄球壳上。
    - 决策论
        - 选择最大后验概率的结果
        - 最小化错误分类概率
        - 最小化期望损失
        - 拒绝选项：置信度低时，拒绝识别样本
        - 推断和决策：inference and decision
            - 生成式模型：对P（x，y）建模，然后计算后验概率，最后用决策论判定。
                - 需要求解的东西最多，但可以计算边缘概率p（x）（对于异常点检测有用）。
            - 判别式模型：直接推断后验概率P（y|x），之后用决策论判定。
                - 只是用于分类的话，直接求后验概率更简单。
            - 直接映射：根据x直接输出结果，结合推断和决策，很难在不考虑后验概率的情况下进行决策。
            - 频率学派与贝叶斯学派是两种学派对于概率的看法不同而分的派系，参数是定值还是概率分布，两者之间没有必然联系。
        - 回归问题，与上述分类问题类似，有下列三个求解思路
            - 先对 p(x,t)进行建模，然后条件概率密度p(t|x)，最后加权积分得到条件期望。
            - 推断 p(t|x)，再加权积分得到条件期望。
            - 直接确定一个回归函数y(x)
    - 信息论
        - 信息熵，最大熵离散分布是均匀分布，连续变量是⾼斯分布（限制一阶矩/二阶矩/归一化）；离散熵和微分熵。
        - 相对熵（KL散度），凸函数，jensen不等式说明kl散度大于等于0，互信息。 
    
        
- 2 概率分布
    - 二元变量
        - 伯努利分布/二项分布，均值/方差/似然函数/最大似然估计，最大似然可能存在严重过拟合，故需要引入先验概率。
        - 后验分布与先验分布有着同样的函数形式，称为共轭性。
        - Beta分布作为先验概率分布，对于伯努利分布具有共轭性，后验分布同样也是Beta分布；根据观测到的数据，后验分布可以自然的更新。
        - 因此如果选择贝叶斯观点，那么学习过程中的顺序（sequential）⽅法可以⾃然⽽然地得出，它与先验和似然函数的选择⽆关，只取决于数据独⽴同分布的假设。
    - 多项式变量
        - 多项式分布，狄利克雷是多项式分布的共轭先验。
    - 高斯分布
        - 对于⼀个⼀元实值向量，使熵取得最⼤值的是⾼斯分布（限制一阶矩/二阶矩/归一化），这个性质对于多元⾼斯也成⽴。
        - 拉普拉斯提出的中⼼极限定理（central limit theorem）：对于某些温和的情况，⼀组随机变量之和（当然也是随机变量）的概率分布随着和式中项的数量的增加⽽逐渐趋向⾼斯分布。
        - 将高斯分布的协方差矩阵转化为特征向量求和形式，可以将高斯分布的形式转化为D个独⽴⼀元⾼斯分布的乘积。
        - 条件高斯分布/边缘高斯分布
            - 多元⾼斯分布的⼀个重要性质是，如果两组变量是联合⾼斯分布，那么以⼀组变量为条件，另⼀组变量同样是⾼斯分布。类似地，任何⼀个变量的边缘分布也是⾼斯分布。
        - 高斯变量的贝叶斯定理
            - 给定x的⼀个边缘⾼斯分布，以及在给定x的条件下y的条件⾼斯分布，可以得到y的边缘高斯分布以及给定y的条件下x的条件高斯分布。
        - 高斯分布的最大似然估计
            - 最大似然估计结果的均值是无偏估计，方差是偏小的。
        - 顺序估计
            - 将最大似然估计看作样本粒度的计算，观测到新样本之后进行更新。
        - ⾼斯分布的贝叶斯推断
            - 后验概率正比于先验函数*似然函数
            - 高斯函数作为先验分布，是高斯分布似然函数的共轭分布，后验还是高斯分布似然函数的形式。
            - 若观测样本数量为0，则推断均值为先验分布均值；若观测样本数量接近无穷，则推断均值为最大似然估计均值；因此从顺序估计的角度来看，贝叶斯观点是十分自然的。
            - 推断方差也是类似的思路。
        - 学⽣t分布
            - 无限个同均值不同方差的高斯分布求和得到，即无限混合高斯模型。
            - 这个分布通常有着⽐⾼斯分布更长的“尾巴”，这给出了t分布的⼀个重要性质：鲁棒性（robustness），意思是对于数据集⾥的⼏个离群点outlier的出现，t分布不会像⾼斯分布那样敏感。
        - 周期变量
            - ⾼斯分布在实际应⽤中⾮常重要，但有些情况下使⽤⾼斯分布建模并不合适，⼀个重要的情况是周期变量。
            - 虑⾼斯分布对于周期变量的⼀个推⼴（von Mises分布）或者环形正态分布（circular normal）。
        - 混合⾼斯模型
            - 通过将更基本的概率分布（例如⾼斯分布）进⾏线性组合得到混合模型。
            - 高斯混合模型利用EM算法求解，在第9章中会详细讨论。
    - 指数族分布
        - 最⼤似然与充分统计量
        - 共轭先验
        - ⽆信息先验
    - ⾮参数化⽅法
        - 核密度估计
        - 近邻⽅法
   

- 3 线性回归
    - 线性基函数模型
    - 最大似然与最小平方（奇异矩阵，参数很大，通过加正则项来解决）
    - 偏置方差
    - 贝叶斯线性回归（先验分布、似然分布、后验分布和预测分布）
    - 最小二乘与几何意义
        - 最小二乘推导
        - 两种几何解释
            - 最小化误差距离
            - 在x构成的空间内找最接近y的函数（X和Y-aX垂直）
    - 最大似然角度推导
    - 正则化，频率/贝叶斯 两个角度的推导
    - LSE（最小二乘）== MLE（高斯噪声）
    - Regularized LES == MAP（贝叶斯回归，高斯先验，高斯噪声）
    
- 4 线性分类
    - 频率派：统计机器学习，贝叶斯派：概率图模型
    - 线性
        - 属性非线性-多项式回归
        - 全局非线性-0/1分类
        - 系数非线性-神经网络
    - 全局性
        - 决策树
        - 线性样条回归
    - 数据加工
        - PCA
        - 流形
    - 线性分类
        - 硬分类，0或1
            - svm
            - 感知机
            - 线性判别分析，fisher
    - 软分类，0～1取值
        - 生成式，高斯判别分析（连续），朴素贝叶斯（离散）
        - 判别式，lr
    - 二分类与多分类
    - 分类中的最小平方
    - fisher线性判别函数（确定投影方向），与最小平方一致
    - 感知器算法

- 5 神经网络

- 6 核方法

- 降维
    - 直接降维（特征选择）
    - 线性降维，PCA，MDS
    - 非线性降维，流形（LLE局部线性嵌入，soimap）

- 7 SVM
    - 二次凸优化问题
    - 约束优化问题——对偶问题
    

- 8 概率图模型
    - 概率图结构，tail-to-tail/tail-to-head/head-to-head
        - tail-to-tail/tail-to-head，节点已知则路径两侧独立。
        - head-to-head，节点与子节点未知则路径两侧独立。

- 9 EM

- 10 近似推断

- 11 采样方法
    - 精确推断不可行，故使用采样方法；虽然我们感兴趣的是⾮观测变量上的后验概率分布本⾝，但是在⼤部分情况下后验概率分布的主要⽤途是计算期望，例如做预测的场景。
    - 概率密度转换公式，利用简单分布采样能力对复杂分布进行采样。
    - 拒绝采样：已知概率密度相对值，但归一化系数难以计算的情况；为了降低拒绝概率，proposal分布应该尽可能的接近目标分布；proposal分布选择难度高，可以利用切线来构造；拒绝采样不适合高维空间。
    - 重要性采样：通过一个相似的proposal简单分布采样来近似目标分布，其中对不同的采样值进行了重要性权重的修正。
    - 蒙特卡罗EM算法：EM中E步骤无法解析计算的模型用采样方法来近似。
    - MCMC
        - 蒙特卡洛方法：通过采样样本来近似代表所有样本，当采样样本数量足够大时，采样样本就可以近似全部样本；蒙特卡洛方法求圆周率。
        - 随机过程：随时间变化的一系列随机变量。
        - 马尔可夫性质：t+1状态的取值只取决于t状态。
        - 马尔可夫过程：具有马尔可夫性质的随机过程成为马尔可夫过程，也可定义为连续指数集的马尔可夫链。
        - 平稳性分布性质：系统在随机时间所在状态的概率是确定的，即平稳分布，也成为静态分布。
        - 马尔可夫链蒙特卡洛方法（MCMC）：是一类以期望分布为平稳分布的马尔可夫链为基础，对概率分布进行抽样的算法。
        - 整体思路：为了对后验分布进行采样，构建一个马尔可夫链（利用似然比来作为接受率，然后用蒙特卡洛方法进行采样），其稳态的分布与后验分布一致。
        - 吉布斯采样：用于在难以直接采样时从某一多变量概率分布中近似抽取样本序列。

- 12 连续潜在变量

- 13 顺序数据

- 14 组合模型